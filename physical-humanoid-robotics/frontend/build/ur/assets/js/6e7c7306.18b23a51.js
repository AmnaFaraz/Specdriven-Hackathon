"use strict";(globalThis.webpackChunkphysical_humanoid_robotics_book=globalThis.webpackChunkphysical_humanoid_robotics_book||[]).push([[478],{8004(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"modules/module-5/module-5-chapter-3","title":"AI Perception and Decision Making","description":"This chapter explores the AI systems that enable the humanoid robot to perceive its environment, understand complex situations, and make intelligent decisions based on multimodal input.","source":"@site/content/modules/module-5/chapter-3.md","sourceDirName":"modules/module-5","slug":"/modules/module-5/module-5-chapter-3","permalink":"/ur/docs/modules/module-5/module-5-chapter-3","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-humanoid-robotics/physical-humanoid-robotics-book/tree/main/packages/create-docusaurus/templates/shared/content/modules/module-5/chapter-3.md","tags":[],"version":"current","frontMatter":{"id":"module-5-chapter-3","title":"AI Perception and Decision Making","sidebar_label":"AI Perception"},"sidebar":"tutorialSidebar","previous":{"title":"Control Systems","permalink":"/ur/docs/modules/module-5/module-5-chapter-2"},"next":{"title":"Integration & Testing","permalink":"/ur/docs/modules/module-5/module-5-chapter-4"}}');var o=t(4848),a=t(8453);const s={id:"module-5-chapter-3",title:"AI Perception and Decision Making",sidebar_label:"AI Perception"},r="AI Perception and Decision Making",c={},l=[{value:"Perception Architecture",id:"perception-architecture",level:2},{value:"Computer Vision for Robotics",id:"computer-vision-for-robotics",level:2},{value:"Multi-Modal Vision Processing",id:"multi-modal-vision-processing",level:3},{value:"3D Scene Understanding",id:"3d-scene-understanding",level:3},{value:"Decision Making Systems",id:"decision-making-systems",level:2},{value:"Hierarchical Decision Making",id:"hierarchical-decision-making",level:3},{value:"Reinforcement Learning for Decision Making",id:"reinforcement-learning-for-decision-making",level:2},{value:"Deep Reinforcement Learning for Robotics",id:"deep-reinforcement-learning-for-robotics",level:3},{value:"Exercise: Implement AI Decision System",id:"exercise-implement-ai-decision-system",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"ai-perception-and-decision-making",children:"AI Perception and Decision Making"})}),"\n",(0,o.jsx)(e.p,{children:"This chapter explores the AI systems that enable the humanoid robot to perceive its environment, understand complex situations, and make intelligent decisions based on multimodal input."}),"\n",(0,o.jsx)(e.h2,{id:"perception-architecture",children:"Perception Architecture"}),"\n",(0,o.jsx)(e.p,{children:"The humanoid robot's perception system processes multiple sensory modalities to build a comprehensive understanding of its environment:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Perception Architecture                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Sensory Processing Layer                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Visual        \u2502   Auditory      \u2502   Tactile & Proprio     \u2502 \u2502\n\u2502  \u2502   (Cameras,     \u2502   (Microphones, \u2502   (IMU, Joint Encoders, \u2502 \u2502\n\u2502  \u2502   Depth, LIDAR) \u2502   Speakers)     \u2502   Force Sensors)       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                 \u2502\n\u2502  Feature Extraction Layer                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Object Detection  \u2502  Semantic Mapping  \u2502  Activity Recog  \u2502 \u2502\n\u2502  \u2502  Recognition       \u2502  Scene Understanding\u2502  Human Behavior  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                 \u2502\n\u2502  Scene Understanding Layer                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  3D Reconstruction \u2502  Spatial Reasoning  \u2502  Temporal Model  \u2502 \u2502\n\u2502  \u2502  SLAM & Mapping   \u2502  Navigation Planning \u2502  Event Tracking  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                 \u2502\n\u2502  Decision Making Layer                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Task Planning      \u2502  Action Selection   \u2502  Behavior       \u2502 \u2502\n\u2502  \u2502  Motion Planning    \u2502  Policy Learning    \u2502  Coordination   \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h2,{id:"computer-vision-for-robotics",children:"Computer Vision for Robotics"}),"\n",(0,o.jsx)(e.h3,{id:"multi-modal-vision-processing",children:"Multi-Modal Vision Processing"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torchvision import transforms\nfrom transformers import CLIPVisionModel, CLIPProcessor\nimport cv2\n\nclass MultiModalVisionProcessor(nn.Module):\n    def __init__(self, clip_model_name="openai/clip-vit-base-patch32"):\n        super().__init__()\n\n        # CLIP for vision-language alignment\n        self.clip_vision = CLIPVisionModel.from_pretrained(clip_model_name)\n        self.clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n\n        # Feature extraction layers\n        self.visual_feature_extractor = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n\n        # Object detection head\n        self.object_detection_head = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 80)  # COCO dataset classes\n        )\n\n        # Semantic segmentation head\n        self.semantic_segmentation_head = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(64, 50, kernel_size=4, stride=2, padding=1),  # 50 semantic classes\n            nn.Sigmoid()\n        )\n\n        # Feature projection for fusion\n        self.feature_projection = nn.Linear(512, 768)  # Project to CLIP dimension\n\n    def forward(self, pixel_values, texts=None):\n        """Process visual input and extract features"""\n        # Extract visual features\n        vision_features = self.clip_vision(pixel_values=pixel_values)\n        vision_features = vision_features.pooler_output\n        vision_features = self.feature_projection(vision_features)\n        vision_features = nn.functional.normalize(vision_features, dim=-1)\n\n        # Object detection\n        object_logits = self.object_detection_head(vision_features)\n\n        # Semantic segmentation\n        visual_features_for_seg = self.visual_feature_extractor(pixel_values)\n        segmentation_maps = self.semantic_segmentation_head(visual_features_for_seg)\n\n        results = {\n            \'vision_features\': vision_features,\n            \'object_detection\': object_logits,\n            \'segmentation_maps\': segmentation_maps,\n            \'clip_features\': vision_features\n        }\n\n        # If texts provided, compute vision-language alignment\n        if texts is not None:\n            inputs = self.clip_processor(text=texts, images=[None]*len(texts), return_tensors="pt", padding=True)\n            text_outputs = self.clip_vision.get_text_features(input_ids=inputs[\'input_ids\'])\n            text_features = nn.functional.normalize(text_outputs, dim=-1)\n\n            # Compute similarity\n            similarity = torch.matmul(vision_features, text_features.t())\n            results[\'alignment_scores\'] = similarity\n\n        return results\n\n    def detect_objects(self, image_tensor):\n        """Detect objects in the image"""\n        with torch.no_grad():\n            results = self.forward(image_tensor)\n            object_probs = torch.softmax(results[\'object_detection\'], dim=-1)\n            return object_probs\n\n    def segment_scene(self, image_tensor):\n        """Perform semantic segmentation of the scene"""\n        with torch.no_grad():\n            results = self.forward(image_tensor)\n            return results[\'segmentation_maps\']\n\n    def align_with_text(self, image_tensor, text_queries):\n        """Align visual features with text queries"""\n        with torch.no_grad():\n            results = self.forward(image_tensor, text_queries)\n            return results[\'alignment_scores\']\n'})}),"\n",(0,o.jsx)(e.h3,{id:"3d-scene-understanding",children:"3D Scene Understanding"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\nfrom scipy.spatial import distance\nimport open3d as o3d\n\nclass SceneUnderstanding3D(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # 3D feature extraction\n        self.pointnet = PointNet3D()\n        self.voxel_encoder = VoxelEncoder()\n        self.surface_normal_estimator = SurfaceNormalEstimator()\n\n        # Spatial reasoning\n        self.spatial_reasoner = SpatialReasoningNetwork()\n\n        # Object relationship modeling\n        self.relationship_predictor = RelationshipPredictor()\n\n    def forward(self, point_cloud, camera_intrinsics):\n        """Process 3D point cloud for scene understanding"""\n        # Extract 3D features\n        point_features = self.pointnet(point_cloud)\n        voxel_features = self.voxel_encoder(point_cloud)\n\n        # Estimate surface normals\n        surface_normals = self.surface_normal_estimator(point_cloud)\n\n        # Extract object proposals\n        object_proposals = self.extract_object_proposals(point_cloud, point_features)\n\n        # Compute spatial relationships\n        spatial_relations = self.spatial_reasoner.compute_relationships(\n            object_proposals, camera_intrinsics\n        )\n\n        # Predict object relationships\n        relationships = self.relationship_predictor(\n            object_proposals, spatial_relations\n        )\n\n        return {\n            \'point_features\': point_features,\n            \'voxel_features\': voxel_features,\n            \'surface_normals\': surface_normals,\n            \'object_proposals\': object_proposals,\n            \'spatial_relations\': spatial_relations,\n            \'relationships\': relationships\n        }\n\n    def extract_object_proposals(self, point_cloud, features):\n        """Extract object proposals from point cloud"""\n        # Use clustering algorithms to group points into objects\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(point_cloud.cpu().numpy())\n\n        # Apply DBSCAN clustering\n        labels = np.array(pcd.cluster_dbscan(eps=0.02, min_points=10))\n\n        # Group points by cluster\n        objects = []\n        for label in set(labels):\n            if label == -1:  # Skip noise points\n                continue\n            mask = labels == label\n            obj_points = np.asarray(pcd.points)[mask]\n            obj_feature = features[mask].mean(dim=0)\n            obj_center = obj_points.mean(dim=0)\n\n            objects.append({\n                \'points\': obj_points,\n                \'features\': obj_feature,\n                \'center\': obj_center,\n                \'label\': label\n            })\n\n        return objects\n\n    def compute_spatial_relations(self, objects, camera_intrinsics):\n        """Compute spatial relationships between objects"""\n        relations = []\n        for i, obj1 in enumerate(objects):\n            for j, obj2 in enumerate(objects):\n                if i != j:\n                    # Compute distance and direction\n                    dist = torch.norm(obj1[\'center\'] - obj2[\'center\'])\n                    direction = (obj2[\'center\'] - obj1[\'center\']) / dist\n\n                    # Project to camera frame to get spatial relationship\n                    rel_type = self.classify_spatial_relation(\n                        obj1[\'center\'], obj2[\'center\'], camera_intrinsics\n                    )\n\n                    relations.append({\n                        \'subject\': i,\n                        \'object\': j,\n                        \'distance\': dist,\n                        \'direction\': direction,\n                        \'relation_type\': rel_type\n                    })\n\n        return relations\n\n    def classify_spatial_relation(self, obj1_center, obj2_center, camera_intrinsics):\n        """Classify spatial relationship between objects"""\n        # Convert to camera coordinates\n        rel_vector = obj2_center - obj1_center\n\n        # Determine spatial relationship based on direction\n        if rel_vector[2] > 0:  # obj2 is in front of obj1\n            if torch.abs(rel_vector[0]) > torch.abs(rel_vector[1]):\n                return "right_of" if rel_vector[0] > 0 else "left_of"\n            else:\n                return "above" if rel_vector[1] > 0 else "below"\n        else:  # obj2 is behind obj1\n            return "behind"\n\nclass PointNet3D(nn.Module):\n    def __init__(self, feature_dim=512):\n        super().__init__()\n        self.feature_dim = feature_dim\n\n        # PointNet architecture\n        self.mlp1 = nn.Sequential(\n            nn.Linear(3, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(inplace=True)\n        )\n\n        self.mlp2 = nn.Sequential(\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, self.feature_dim),\n            nn.BatchNorm1d(self.feature_dim),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, point_cloud):\n        """Process point cloud through PointNet"""\n        # Input: (batch_size, num_points, 3)\n        batch_size, num_points, _ = point_cloud.shape\n\n        # Apply MLP to each point\n        point_features = self.mlp1(point_cloud.view(-1, 3))\n        point_features = point_features.view(batch_size, num_points, -1)\n\n        # Global feature aggregation (max pooling)\n        global_features = torch.max(point_features, dim=1)[0]\n\n        # Final feature extraction\n        final_features = self.mlp2(global_features)\n\n        return final_features\n\nclass VoxelEncoder(nn.Module):\n    def __init__(self, grid_size=32):\n        super().__init__()\n        self.grid_size = grid_size\n\n        # Voxel-based 3D CNN\n        self.cnn3d = nn.Sequential(\n            nn.Conv3d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm3d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool3d((1, 1, 1)),\n            nn.Flatten(),\n            nn.Linear(128, 512)\n        )\n\n    def forward(self, point_cloud):\n        """Process point cloud through voxel encoding"""\n        # Convert point cloud to voxel grid\n        voxel_grid = self.point_cloud_to_voxel_grid(point_cloud)\n\n        # Apply 3D CNN\n        features = self.cnn3d(voxel_grid)\n\n        return features\n\n    def point_cloud_to_voxel_grid(self, point_cloud):\n        """Convert point cloud to voxel grid representation"""\n        # Normalize point cloud to grid bounds\n        min_vals = torch.min(point_cloud, dim=1, keepdim=True)[0]\n        max_vals = torch.max(point_cloud, dim=1, keepdim=True)[0]\n        normalized_points = (point_cloud - min_vals) / (max_vals - min_vals + 1e-6)\n\n        # Scale to voxel grid size\n        scaled_points = normalized_points * (self.grid_size - 1)\n\n        # Quantize to voxel indices\n        voxel_indices = torch.floor(scaled_points).long()\n\n        # Create voxel grid\n        batch_size = point_cloud.shape[0]\n        voxel_grid = torch.zeros((batch_size, 1, self.grid_size, self.grid_size, self.grid_size))\n\n        # Fill voxel grid\n        for b in range(batch_size):\n            valid_mask = (voxel_indices[b] >= 0) & (voxel_indices[b] < self.grid_size)\n            valid_mask = torch.all(valid_mask, dim=1)\n            valid_indices = voxel_indices[b][valid_mask]\n\n            # Fill occupied voxels\n            voxel_grid[b, 0, valid_indices[:, 0], valid_indices[:, 1], valid_indices[:, 2]] = 1.0\n\n        return voxel_grid\n'})}),"\n",(0,o.jsx)(e.h2,{id:"decision-making-systems",children:"Decision Making Systems"}),"\n",(0,o.jsx)(e.h3,{id:"hierarchical-decision-making",children:"Hierarchical Decision Making"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom collections import defaultdict\nimport heapq\n\nclass HierarchicalDecisionMaker(nn.Module):\n    def __init__(self, num_tasks=10, action_dim=7):\n        super().__init__()\n\n        # High-level task planner\n        self.task_planner = TaskPlanner(num_tasks)\n\n        # Mid-level motion planner\n        self.motion_planner = MotionPlanner(action_dim)\n\n        # Low-level action executor\n        self.action_executor = ActionExecutor(action_dim)\n\n        # Task-goal alignment\n        self.task_alignment = TaskGoalAlignment()\n\n        # Memory for context\n        self.context_memory = ContextMemory()\n\n    def forward(self, perception_input, goal_specification):\n        \"\"\"Make hierarchical decisions based on perception and goals\"\"\"\n        # Update context memory\n        self.context_memory.update(perception_input)\n\n        # High-level task planning\n        task_plan = self.task_planner(plan_context={\n            'perception': perception_input,\n            'goal': goal_specification,\n            'memory': self.context_memory.get_recent_context()\n        })\n\n        # Mid-level motion planning\n        motion_plan = self.motion_planner(plan_context={\n            'task_plan': task_plan,\n            'perception': perception_input,\n            'obstacles': perception_input.get('obstacles', []),\n            'targets': perception_input.get('targets', [])\n        })\n\n        # Low-level action execution\n        action = self.action_executor.execute_context={\n            'motion_plan': motion_plan,\n            'perception': perception_input,\n            'task_plan': task_plan\n        })\n\n        return {\n            'task_plan': task_plan,\n            'motion_plan': motion_plan,\n            'action': action,\n            'confidence': self.assess_confidence(perception_input, action)\n        }\n\n    def assess_confidence(self, perception_input, action):\n        \"\"\"Assess confidence in the decision\"\"\"\n        # Evaluate based on sensor quality, obstacle density, etc.\n        confidence = 1.0\n\n        # Reduce confidence if sensor data is poor\n        if perception_input.get('sensor_quality', 1.0) < 0.5:\n            confidence *= 0.8\n\n        # Reduce confidence if many obstacles nearby\n        obstacles = perception_input.get('obstacles', [])\n        if len(obstacles) > 5:\n            confidence *= 0.7\n\n        # Reduce confidence if action is complex\n        action_complexity = torch.norm(action).item()\n        if action_complexity > 0.8:\n            confidence *= 0.9\n\n        return confidence\n\nclass TaskPlanner(nn.Module):\n    def __init__(self, num_tasks):\n        super().__init__()\n        self.num_tasks = num_tasks\n\n        # Task selection network\n        self.task_selector = nn.Sequential(\n            nn.Linear(512, 256),  # Input features from perception\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_tasks),\n            nn.Softmax(dim=-1)\n        )\n\n        # Task dependency graph\n        self.task_dependencies = self.initialize_task_dependencies()\n\n    def forward(self, plan_context):\n        \"\"\"Plan high-level tasks\"\"\"\n        perception_features = plan_context['perception']['features']\n        goal_features = plan_context['goal']['features']\n\n        # Combine perception and goal features\n        combined_features = torch.cat([perception_features, goal_features], dim=-1)\n\n        # Select tasks\n        task_probs = self.task_selector(combined_features)\n        selected_task_idx = torch.argmax(task_probs, dim=-1)\n\n        # Generate task sequence considering dependencies\n        task_sequence = self.generate_task_sequence(selected_task_idx, plan_context['memory'])\n\n        return {\n            'selected_task': selected_task_idx,\n            'task_probs': task_probs,\n            'sequence': task_sequence,\n            'dependencies': self.task_dependencies[selected_task_idx.item()]\n        }\n\n    def initialize_task_dependencies(self):\n        \"\"\"Initialize task dependency graph\"\"\"\n        # Example dependencies: [walk, grasp, manipulate, avoid_obstacle, ...]\n        dependencies = {\n            0: [],  # walk\n            1: [0],  # grasp requires walk to reach object\n            2: [1],  # manipulate requires grasp\n            3: [0],  # avoid_obstacle may interrupt other tasks\n            # Add more dependencies as needed\n        }\n        return dependencies\n\n    def generate_task_sequence(self, initial_task, memory):\n        \"\"\"Generate sequence of tasks considering dependencies\"\"\"\n        # Use topological sort to respect dependencies\n        visited = set()\n        sequence = []\n\n        def dfs(task_idx):\n            if task_idx in visited:\n                return\n            visited.add(task_idx)\n\n            # Visit dependencies first\n            for dep in self.task_dependencies.get(task_idx, []):\n                dfs(dep)\n\n            sequence.append(task_idx)\n\n        dfs(initial_task.item())\n        return sequence\n\nclass MotionPlanner(nn.Module):\n    def __init__(self, action_dim):\n        super().__init__()\n        self.action_dim = action_dim\n\n        # Path planning network\n        self.path_planner = nn.Sequential(\n            nn.Linear(256, 512),  # Input: [start, goal, obstacles]\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        )\n\n        # Obstacle avoidance\n        self.obstacle_avoider = ObstacleAvoidanceNetwork()\n\n    def forward(self, plan_context):\n        \"\"\"Plan motion trajectories\"\"\"\n        task_plan = plan_context['task_plan']\n        perception = plan_context['perception']\n        obstacles = plan_context['obstacles']\n        targets = plan_context['targets']\n\n        # Plan path to target\n        path = self.plan_path_to_target(targets, obstacles)\n\n        # Generate motion commands\n        motion_commands = self.generate_motion_commands(path, task_plan)\n\n        return {\n            'path': path,\n            'commands': motion_commands,\n            'avoidance_strategy': self.obstacle_avoider(obstacles)\n        }\n\n    def plan_path_to_target(self, targets, obstacles):\n        \"\"\"Plan path to target using A* or RRT\"\"\"\n        # Simplified path planning\n        if len(targets) > 0:\n            target = targets[0]  # Use first target\n            # In practice, this would use A* or RRT algorithm\n            path = [target]  # Simplified\n        else:\n            path = []\n\n        return path\n\n    def generate_motion_commands(self, path, task_plan):\n        \"\"\"Generate motion commands for the path\"\"\"\n        commands = []\n        for waypoint in path:\n            # Generate command to move to waypoint\n            command = self.path_planner(torch.cat([waypoint, task_plan['selected_task'].float()]))\n            commands.append(command)\n        return commands\n\nclass ActionExecutor(nn.Module):\n    def __init__(self, action_dim):\n        super().__init__()\n        self.action_dim = action_dim\n\n        # Action selection network\n        self.action_selector = nn.Sequential(\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim),\n            nn.Tanh()  # Actions in [-1, 1]\n        )\n\n    def forward(self, execute_context):\n        \"\"\"Execute the planned action\"\"\"\n        motion_plan = execute_context['motion_plan']\n        perception = execute_context['perception']\n        task_plan = execute_context['task_plan']\n\n        # Select appropriate action based on context\n        context_features = torch.cat([\n            motion_plan['commands'][0] if motion_plan['commands'] else torch.zeros(self.action_dim),\n            task_plan['selected_task'].float()\n        ])\n\n        action = self.action_selector(context_features)\n\n        return action\n"})}),"\n",(0,o.jsx)(e.h2,{id:"reinforcement-learning-for-decision-making",children:"Reinforcement Learning for Decision Making"}),"\n",(0,o.jsx)(e.h3,{id:"deep-reinforcement-learning-for-robotics",children:"Deep Reinforcement Learning for Robotics"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\nimport random\nfrom collections import deque\n\nclass DDPGRobotAgent(nn.Module):\n    def __init__(self, state_dim=512, action_dim=7, hidden_dim=256):\n        super().__init__()\n\n        # Actor network (policy network)\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()  # Actions in [-1, 1]\n        )\n\n        # Critic network (Q-function)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)  # Q-value\n        )\n\n        # Target networks for stable learning\n        self.actor_target = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()\n        )\n        self.critic_target = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # Initialize target networks\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n        # Replay buffer\n        self.replay_buffer = deque(maxlen=100000)\n\n        # Parameters\n        self.action_dim = action_dim\n        self.noise_std = 0.1\n        self.noise_decay = 0.999\n\n    def forward(self, state):\n        """Get action for given state"""\n        action = self.actor(state)\n        return action\n\n    def get_action_with_noise(self, state, add_noise=True):\n        """Get action with optional exploration noise"""\n        action = self.actor(state).detach().cpu().numpy()\n\n        if add_noise:\n            noise = np.random.normal(0, self.noise_std, size=self.action_dim)\n            action = np.clip(action + noise, -1, 1)\n            self.noise_std *= self.noise_decay  # Decay noise over time\n\n        return action\n\n    def update(self, batch_size=64, gamma=0.99, tau=0.005):\n        """Update networks using DDPG algorithm"""\n        if len(self.replay_buffer) < batch_size:\n            return\n\n        # Sample batch from replay buffer\n        batch = random.sample(self.replay_buffer, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(states)\n        actions = torch.FloatTensor(actions)\n        rewards = torch.FloatTensor(rewards).unsqueeze(1)\n        next_states = torch.FloatTensor(next_states)\n        dones = torch.FloatTensor(dones).unsqueeze(1)\n\n        # Update critic\n        next_actions = self.actor_target(next_states)\n        next_q_values = self.critic_target(torch.cat([next_states, next_actions], dim=1))\n        target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n\n        current_q_values = self.critic(torch.cat([states, actions], dim=1))\n\n        critic_loss = nn.MSELoss()(current_q_values, target_q_values.detach())\n\n        # Update actor\n        predicted_actions = self.actor(states)\n        actor_loss = -self.critic(torch.cat([states, predicted_actions], dim=1)).mean()\n\n        # Optimize networks\n        actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)\n        critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=1e-3)\n\n        critic_optimizer.zero_grad()\n        critic_loss.backward()\n        critic_optimizer.step()\n\n        actor_optimizer.zero_grad()\n        actor_loss.backward()\n        actor_optimizer.step()\n\n        # Update target networks\n        self.soft_update(self.actor, self.actor_target, tau)\n        self.soft_update(self.critic, self.critic_target, tau)\n\n        return actor_loss.item(), critic_loss.item()\n\n    def soft_update(self, local_model, target_model, tau):\n        """Soft update model parameters"""\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n\n    def store_transition(self, state, action, reward, next_state, done):\n        """Store transition in replay buffer"""\n        self.replay_buffer.append((state, action, reward, next_state, done))\n\nclass PerceptualRLAgent(nn.Module):\n    def __init__(self, vision_dim=512, proprioceptive_dim=20, action_dim=7):\n        super().__init__()\n\n        # Vision processing\n        self.vision_processor = nn.Sequential(\n            nn.Linear(vision_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU()\n        )\n\n        # Proprioceptive processing\n        self.proprioceptive_processor = nn.Sequential(\n            nn.Linear(proprioceptive_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU()\n        )\n\n        # Combined state processing\n        combined_dim = 128 + 32\n        self.state_processor = nn.Sequential(\n            nn.Linear(combined_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU()\n        )\n\n        # Actor network\n        self.actor = nn.Sequential(\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim),\n            nn.Tanh()\n        )\n\n        # Critic network\n        self.critic = nn.Sequential(\n            nn.Linear(128 + action_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n        # Target networks\n        self.actor_target = nn.Sequential(\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim),\n            nn.Tanh()\n        )\n        self.critic_target = nn.Sequential(\n            nn.Linear(128 + action_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n        # Initialize target networks\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n        # Replay buffer\n        self.replay_buffer = deque(maxlen=50000)\n\n    def forward(self, vision_features, proprioceptive_features):\n        """Process perceptual inputs and generate action"""\n        # Process vision\n        vision_processed = self.vision_processor(vision_features)\n\n        # Process proprioception\n        proprio_processed = self.proprioceptive_processor(proprioceptive_features)\n\n        # Combine features\n        combined_features = torch.cat([vision_processed, proprio_processed], dim=-1)\n        state_features = self.state_processor(combined_features)\n\n        # Generate action\n        action = self.actor(state_features)\n\n        return action\n\n    def get_action(self, vision_features, proprioceptive_features, add_noise=True):\n        """Get action with optional exploration noise"""\n        action = self.forward(vision_features, proprioceptive_features).detach().cpu().numpy()\n\n        if add_noise:\n            noise = np.random.normal(0, 0.1, size=len(action))\n            action = np.clip(action + noise, -1, 1)\n\n        return action\n'})}),"\n",(0,o.jsx)(e.h2,{id:"exercise-implement-ai-decision-system",children:"Exercise: Implement AI Decision System"}),"\n",(0,o.jsx)(e.p,{children:"Create an AI system that:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Processes multimodal perception data (vision, audio, tactile)"}),"\n",(0,o.jsx)(e.li,{children:"Makes hierarchical decisions for complex tasks"}),"\n",(0,o.jsx)(e.li,{children:"Uses reinforcement learning for skill improvement"}),"\n",(0,o.jsx)(e.li,{children:"Integrates with the control system for execution"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"The AI perception and decision-making systems form the cognitive core of the humanoid robot. By processing multiple sensory modalities and making intelligent decisions based on environmental understanding, these systems enable the robot to perform complex autonomous behaviors. The integration of computer vision, 3D scene understanding, hierarchical planning, and reinforcement learning creates a sophisticated AI system capable of navigating and interacting with the real world effectively."}),"\n",(0,o.jsx)(e.hr,{})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>s,x:()=>r});var i=t(6540);const o={},a=i.createContext(o);function s(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);