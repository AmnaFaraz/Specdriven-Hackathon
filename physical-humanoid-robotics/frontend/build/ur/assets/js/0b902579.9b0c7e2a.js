"use strict";(globalThis.webpackChunkphysical_humanoid_robotics_book=globalThis.webpackChunkphysical_humanoid_robotics_book||[]).push([[317],{6470(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>c,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>r});const i=JSON.parse('{"id":"modules/module-4/module-4-chapter-3","title":"Action Generation and Execution in VLA Systems","description":"This chapter explores how Vision-Language-Action (VLA) systems generate and execute physical actions based on multimodal input, bridging the gap between perception and action in robotic systems.","source":"@site/content/modules/module-4/chapter-3.md","sourceDirName":"modules/module-4","slug":"/modules/module-4/module-4-chapter-3","permalink":"/ur/docs/modules/module-4/module-4-chapter-3","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-humanoid-robotics/physical-humanoid-robotics-book/tree/main/packages/create-docusaurus/templates/shared/content/modules/module-4/chapter-3.md","tags":[],"version":"current","frontMatter":{"id":"module-4-chapter-3","title":"Action Generation and Execution in VLA Systems","sidebar_label":"VLA Actions"},"sidebar":"tutorialSidebar","previous":{"title":"VLA Implementation","permalink":"/ur/docs/modules/module-4/module-4-chapter-2"},"next":{"title":"VLA Training","permalink":"/ur/docs/modules/module-4/module-4-chapter-4"}}');var a=t(4848),o=t(8453);const s={id:"module-4-chapter-3",title:"Action Generation and Execution in VLA Systems",sidebar_label:"VLA Actions"},c="Action Generation and Execution in VLA Systems",l={},r=[{value:"Action Space Representation",id:"action-space-representation",level:2},{value:"Continuous Action Spaces",id:"continuous-action-spaces",level:3},{value:"Discrete Action Spaces",id:"discrete-action-spaces",level:3},{value:"Hierarchical Action Planning",id:"hierarchical-action-planning",level:2},{value:"Hierarchical Action Network",id:"hierarchical-action-network",level:3},{value:"Task-Oriented Action Generation",id:"task-oriented-action-generation",level:2},{value:"Task-Based Action Generator",id:"task-based-action-generator",level:3},{value:"Real-time Action Execution",id:"real-time-action-execution",level:2},{value:"Action Execution Manager",id:"action-execution-manager",level:3},{value:"Action Safety and Validation",id:"action-safety-and-validation",level:2},{value:"Action Safety Validator",id:"action-safety-validator",level:3},{value:"Exercise: Implement Safe Action Execution System",id:"exercise-implement-safe-action-execution-system",level:2},{value:"Summary",id:"summary",level:2}];function _(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"action-generation-and-execution-in-vla-systems",children:"Action Generation and Execution in VLA Systems"})}),"\n",(0,a.jsx)(e.p,{children:"This chapter explores how Vision-Language-Action (VLA) systems generate and execute physical actions based on multimodal input, bridging the gap between perception and action in robotic systems."}),"\n",(0,a.jsx)(e.h2,{id:"action-space-representation",children:"Action Space Representation"}),"\n",(0,a.jsx)(e.h3,{id:"continuous-action-spaces",children:"Continuous Action Spaces"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass ContinuousActionSpace(nn.Module):\n    def __init__(self, action_dim=7, low_bounds=None, high_bounds=None):\n        super().__init__()\n\n        self.action_dim = action_dim\n\n        # Define action bounds\n        if low_bounds is None:\n            self.low_bounds = torch.tensor([-1.0] * action_dim)\n        else:\n            self.low_bounds = torch.tensor(low_bounds)\n\n        if high_bounds is None:\n            self.high_bounds = torch.tensor([1.0] * action_dim)\n        else:\n            self.high_bounds = torch.tensor(high_bounds)\n\n    def forward(self, action_logits):\n        """Map action logits to continuous action space"""\n        # Use tanh to map to [-1, 1] range\n        raw_actions = torch.tanh(action_logits)\n\n        # Scale to desired range\n        scale = (self.high_bounds - self.low_bounds) / 2\n        offset = (self.high_bounds + self.low_bounds) / 2\n\n        scaled_actions = raw_actions * scale + offset\n\n        return scaled_actions\n\n    def sample_action(self, mean, std):\n        """Sample action from Gaussian distribution"""\n        noise = torch.randn_like(mean) * std\n        action = mean + noise\n        return torch.clamp(action, self.low_bounds, self.high_bounds)\n\nclass RobotActionGenerator(nn.Module):\n    def __init__(self, vision_dim=512, text_dim=512, action_dim=7):\n        super().__init__()\n\n        # Fusion layer to combine vision and text features\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(vision_dim + text_dim, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU()\n        )\n\n        # Action generation layers\n        self.action_mean = nn.Linear(256, action_dim)\n        self.action_std = nn.Sequential(\n            nn.Linear(256, action_dim),\n            nn.Softplus()  # Ensure positive standard deviation\n        )\n\n        # Action space\n        self.action_space = ContinuousActionSpace(action_dim=action_dim)\n\n    def forward(self, vision_features, text_features):\n        """Generate actions from vision and text features"""\n        # Combine vision and text features\n        combined_features = torch.cat([vision_features, text_features], dim=-1)\n\n        # Pass through fusion network\n        fused_features = self.fusion_layer(combined_features)\n\n        # Generate action parameters\n        action_mean = self.action_mean(fused_features)\n        action_std = self.action_std(fused_features)\n\n        # Sample action from distribution\n        action = self.action_space.sample_action(action_mean, action_std)\n\n        return {\n            \'action\': action,\n            \'mean\': action_mean,\n            \'std\': action_std,\n            \'features\': fused_features\n        }\n'})}),"\n",(0,a.jsx)(e.h3,{id:"discrete-action-spaces",children:"Discrete Action Spaces"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DiscreteActionSpace(nn.Module):\n    def __init__(self, num_actions=20):\n        super().__init__()\n        self.num_actions = num_actions\n\n    def forward(self, action_logits):\n        """Convert action logits to discrete action"""\n        # Apply softmax to get action probabilities\n        action_probs = F.softmax(action_logits, dim=-1)\n\n        # Sample action based on probabilities\n        action_idx = torch.multinomial(action_probs, 1)\n\n        return action_idx, action_probs\n\nclass DiscreteActionGenerator(nn.Module):\n    def __init__(self, vision_dim=512, text_dim=512, num_actions=20):\n        super().__init__()\n\n        # Fusion layer\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(vision_dim + text_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU()\n        )\n\n        # Action prediction head\n        self.action_head = nn.Linear(128, num_actions)\n\n        # Action space\n        self.action_space = DiscreteActionSpace(num_actions=num_actions)\n\n    def forward(self, vision_features, text_features):\n        """Generate discrete action from vision and text features"""\n        # Combine features\n        combined_features = torch.cat([vision_features, text_features], dim=-1)\n\n        # Fuse features\n        fused_features = self.fusion_layer(combined_features)\n\n        # Predict action logits\n        action_logits = self.action_head(fused_features)\n\n        # Get discrete action\n        action_idx, action_probs = self.action_space(action_logits)\n\n        return {\n            \'action_idx\': action_idx,\n            \'action_probs\': action_probs,\n            \'logits\': action_logits\n        }\n'})}),"\n",(0,a.jsx)(e.h2,{id:"hierarchical-action-planning",children:"Hierarchical Action Planning"}),"\n",(0,a.jsx)(e.h3,{id:"hierarchical-action-network",children:"Hierarchical Action Network"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass HierarchicalActionPlanner(nn.Module):\n    def __init__(self, vision_dim=512, text_dim=512, high_level_actions=10, low_level_actions=50):\n        super().__init__()\n\n        # High-level planner (goal-oriented)\n        self.high_level_planner = nn.Sequential(\n            nn.Linear(vision_dim + text_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, high_level_actions)\n        )\n\n        # Low-level executor (motion primitive)\n        self.low_level_executor = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(vision_dim + text_dim + high_level_actions, 256),\n                nn.ReLU(),\n                nn.Linear(256, 128),\n                nn.ReLU(),\n                nn.Linear(128, low_level_actions)\n            ) for _ in range(high_level_actions)\n        ])\n\n        self.high_level_actions = high_level_actions\n        self.low_level_actions = low_level_actions\n\n    def forward(self, vision_features, text_features):\n        \"\"\"Generate hierarchical action plan\"\"\"\n        # Combine vision and text\n        combined_features = torch.cat([vision_features, text_features], dim=-1)\n\n        # High-level planning\n        high_level_logits = self.high_level_planner(combined_features)\n        high_level_probs = F.softmax(high_level_logits, dim=-1)\n        high_level_action = torch.argmax(high_level_probs, dim=-1)\n\n        # Low-level execution based on high-level plan\n        # One-hot encode high-level action\n        high_action_onehot = F.one_hot(high_level_action, num_classes=self.high_level_actions).float()\n\n        # Concatenate with vision and text features\n        low_level_input = torch.cat([combined_features, high_action_onehot], dim=-1)\n\n        # Select appropriate low-level network\n        low_level_logits = []\n        for i in range(self.high_level_actions):\n            # Compute logits for each low-level network\n            logits_i = self.low_level_executor[i](low_level_input)\n            low_level_logits.append(logits_i)\n\n        # Stack and select based on high-level action\n        low_level_logits = torch.stack(low_level_logits, dim=1)  # [batch, high_actions, low_actions]\n\n        # Select logits for the chosen high-level action\n        batch_indices = torch.arange(len(high_level_action))\n        selected_logits = low_level_logits[batch_indices, high_level_action]\n\n        # Get low-level action\n        low_level_probs = F.softmax(selected_logits, dim=-1)\n        low_level_action = torch.argmax(low_level_probs, dim=-1)\n\n        return {\n            'high_level_action': high_level_action,\n            'low_level_action': low_level_action,\n            'high_level_probs': high_level_probs,\n            'low_level_probs': low_level_probs\n        }\n\n    def get_action_sequence(self, vision_features, text_features, sequence_length=5):\n        \"\"\"Generate action sequence for a task\"\"\"\n        actions = []\n\n        for t in range(sequence_length):\n            action_output = self.forward(vision_features, text_features)\n            actions.append({\n                'high': action_output['high_level_action'].item(),\n                'low': action_output['low_level_action'].item(),\n                'high_prob': action_output['high_level_probs'].max().item(),\n                'low_prob': action_output['low_level_probs'].max().item()\n            })\n\n            # Update features based on expected outcome (simplified)\n            # In practice, this would use the actual robot state after each action\n            vision_features = vision_features * 0.95  # Simulate state transition\n\n        return actions\n"})}),"\n",(0,a.jsx)(e.h2,{id:"task-oriented-action-generation",children:"Task-Oriented Action Generation"}),"\n",(0,a.jsx)(e.h3,{id:"task-based-action-generator",children:"Task-Based Action Generator"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass TaskOrientedActionGenerator(nn.Module):\n    def __init__(self, vision_dim=512, text_dim=512, action_dim=7, num_tasks=10):\n        super().__init__()\n\n        # Task classifier\n        self.task_classifier = nn.Sequential(\n            nn.Linear(vision_dim + text_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_tasks)\n        )\n\n        # Task-specific action generators\n        self.task_specific_generators = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(vision_dim + text_dim, 512),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(512, 256),\n                nn.ReLU(),\n                nn.Linear(256, action_dim),\n                nn.Tanh()  # Normalize to [-1, 1]\n            ) for _ in range(num_tasks)\n        ])\n\n        # Task-specific value estimators\n        self.task_value_estimators = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(vision_dim + text_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, 1)\n            ) for _ in range(num_tasks)\n        ])\n\n        self.num_tasks = num_tasks\n        self.action_dim = action_dim\n\n    def forward(self, vision_features, text_features):\n        """Generate task-oriented actions"""\n        # Combine features\n        combined_features = torch.cat([vision_features, text_features], dim=-1)\n\n        # Classify task\n        task_logits = self.task_classifier(combined_features)\n        task_probs = F.softmax(task_logits, dim=-1)\n        task_idx = torch.argmax(task_probs, dim=-1)\n\n        # Generate action for the identified task\n        actions = []\n        values = []\n\n        for i in range(self.num_tasks):\n            # Generate action for each task\n            task_action = self.task_specific_generators[i](combined_features)\n            task_value = self.task_value_estimators[i](combined_features)\n\n            actions.append(task_action)\n            values.append(task_value)\n\n        # Stack actions and values\n        all_actions = torch.stack(actions, dim=1)  # [batch, num_tasks, action_dim]\n        all_values = torch.stack(values, dim=1)    # [batch, num_tasks, 1]\n\n        # Select action based on identified task\n        batch_indices = torch.arange(len(task_idx))\n        selected_actions = all_actions[batch_indices, task_idx]\n        selected_values = all_values[batch_indices, task_idx]\n\n        return {\n            \'task_idx\': task_idx,\n            \'task_probs\': task_probs,\n            \'action\': selected_actions,\n            \'value\': selected_values,\n            \'all_actions\': all_actions,\n            \'all_values\': all_values\n        }\n\n    def get_task_name(self, task_idx):\n        """Get task name from index"""\n        task_names = [\n            "navigation", "grasping", "manipulation", "inspection",\n            "transport", "assembly", "disassembly", "cleaning",\n            "monitoring", "communication"\n        ]\n        return task_names[task_idx] if task_idx < len(task_names) else "unknown"\n'})}),"\n",(0,a.jsx)(e.h2,{id:"real-time-action-execution",children:"Real-time Action Execution"}),"\n",(0,a.jsx)(e.h3,{id:"action-execution-manager",children:"Action Execution Manager"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Image\nfrom geometry_msgs.msg import Twist, Pose\nfrom std_msgs.msg import String, Float32MultiArray\nfrom cv_bridge import CvBridge\nimport torch\nimport time\nimport numpy as np\n\nclass ActionExecutionManager(Node):\n    def __init__(self):\n        super().__init__(\'action_execution_manager\')\n\n        # Initialize VLA components\n        self.action_generator = TaskOrientedActionGenerator()\n        self.action_generator.eval()\n\n        # ROS 2 interfaces\n        self.joint_state_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_state_callback, 10\n        )\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_rect_color\', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, \'/vla_command\', self.command_callback, 10\n        )\n\n        # Action publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.joint_cmd_pub = self.create_publisher(JointState, \'/joint_commands\', 10)\n        self.pose_cmd_pub = self.create_publisher(Pose, \'/pose_command\', 10)\n\n        # Processing components\n        self.bridge = CvBridge()\n        self.current_joint_state = None\n        self.current_image = None\n        self.pending_command = None\n\n        # Action execution parameters\n        self.execution_frequency = 10  # Hz\n        self.action_timeout = 5.0  # seconds\n        self.last_action_time = time.time()\n\n    def joint_state_callback(self, msg):\n        """Update joint state"""\n        self.current_joint_state = msg\n\n    def image_callback(self, msg):\n        """Process incoming image"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n            self.current_image = cv_image\n\n            # Process if we have a pending command\n            if self.pending_command:\n                self.execute_vla_action()\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def command_callback(self, msg):\n        """Process incoming command"""\n        self.pending_command = msg.data\n\n        # Process if we have current image\n        if self.current_image:\n            self.execute_vla_action()\n\n    def execute_vla_action(self):\n        """Execute VLA-generated action"""\n        if not self.current_image or not self.pending_command:\n            return\n\n        try:\n            # Preprocess inputs\n            image_tensor = self.preprocess_image(self.current_image)\n            text_tensor = self.preprocess_text(self.pending_command)\n\n            # Generate action using VLA model\n            with torch.no_grad():\n                action_output = self.action_generator(image_tensor, text_tensor)\n\n            # Extract action\n            action = action_output[\'action\'].cpu().numpy()[0]\n            task_idx = action_output[\'task_idx\'].cpu().numpy()[0]\n\n            # Execute action based on task type\n            self.execute_task_specific_action(action, task_idx)\n\n            # Update execution time\n            self.last_action_time = time.time()\n\n            # Clear processed command\n            self.pending_command = None\n\n        except Exception as e:\n            self.get_logger().error(f\'Error executing VLA action: {e}\')\n\n    def preprocess_image(self, image):\n        """Preprocess image for VLA model"""\n        import cv2\n        resized = cv2.resize(image, (224, 224))\n        normalized = resized.astype(np.float32) / 255.0\n        tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)\n        return tensor\n\n    def preprocess_text(self, text):\n        """Preprocess text for VLA model"""\n        from transformers import CLIPTokenizer\n        tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")\n        inputs = tokenizer(\n            text,\n            return_tensors="pt",\n            padding=True,\n            truncation=True,\n            max_length=77\n        )\n        return inputs[\'input_ids\']\n\n    def execute_task_specific_action(self, action, task_idx):\n        """Execute action based on task type"""\n        if task_idx == 0:  # Navigation\n            self.execute_navigation_action(action)\n        elif task_idx == 1:  # Grasping\n            self.execute_grasping_action(action)\n        elif task_idx == 2:  # Manipulation\n            self.execute_manipulation_action(action)\n        else:  # Default action\n            self.execute_default_action(action)\n\n    def execute_navigation_action(self, action):\n        """Execute navigation action"""\n        cmd_vel = Twist()\n\n        # Map action to navigation commands\n        cmd_vel.linear.x = action[0]  # Forward/backward\n        cmd_vel.angular.z = action[1]  # Turn left/right\n\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info(f\'Navigating: linear={cmd_vel.linear.x:.2f}, angular={cmd_vel.angular.z:.2f}\')\n\n    def execute_grasping_action(self, action):\n        """Execute grasping action"""\n        joint_cmd = JointState()\n        joint_cmd.name = [\'finger_joint_1\', \'finger_joint_2\']  # Example joint names\n        joint_cmd.position = [action[0], action[1]]  # Finger positions\n\n        self.joint_cmd_pub.publish(joint_cmd)\n        self.get_logger().info(f\'Grasping: fingers={[f"{pos:.2f}" for pos in joint_cmd.position]}\')\n\n    def execute_manipulation_action(self, action):\n        """Execute manipulation action"""\n        pose_cmd = Pose()\n\n        # Map action to end-effector pose\n        pose_cmd.position.x = action[0]\n        pose_cmd.position.y = action[1]\n        pose_cmd.position.z = action[2]\n        pose_cmd.orientation.z = action[3]\n        pose_cmd.orientation.w = action[4]\n\n        self.pose_cmd_pub.publish(pose_cmd)\n        self.get_logger().info(f\'Manipulating: pos=({pose_cmd.position.x:.2f}, {pose_cmd.position.y:.2f}, {pose_cmd.position.z:.2f})\')\n\n    def execute_default_action(self, action):\n        """Execute default action"""\n        cmd_vel = Twist()\n        cmd_vel.linear.x = action[0] * 0.2  # Scale down for safety\n        cmd_vel.angular.z = action[1] * 0.2\n\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info(f\'Default action: linear={cmd_vel.linear.x:.2f}, angular={cmd_vel.angular.z:.2f}\')\n\n    def check_action_timeout(self):\n        """Check if action has timed out"""\n        if time.time() - self.last_action_time > self.action_timeout:\n            # Stop robot if action times out\n            cmd_vel = Twist()\n            self.cmd_vel_pub.publish(cmd_vel)\n            self.get_logger().warn(\'Action timed out, stopping robot\')\n'})}),"\n",(0,a.jsx)(e.h2,{id:"action-safety-and-validation",children:"Action Safety and Validation"}),"\n",(0,a.jsx)(e.h3,{id:"action-safety-validator",children:"Action Safety Validator"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass ActionSafetyValidator(nn.Module):\n    def __init__(self, action_dim=7, safety_threshold=0.8):\n        super().__init__()\n\n        # Safety critic network\n        self.safety_critic = nn.Sequential(\n            nn.Linear(action_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Sigmoid()  # Output safety probability [0, 1]\n        )\n\n        # Action constraint networks\n        self.position_constraint = nn.Sequential(\n            nn.Linear(action_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Sigmoid()\n        )\n\n        self.velocity_constraint = nn.Sequential(\n            nn.Linear(action_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1),\n            nn.Sigmoid()\n        )\n\n        self.safety_threshold = safety_threshold\n\n    def forward(self, action, current_state=None):\n        \"\"\"Validate action safety\"\"\"\n        # Check action safety\n        safety_score = self.safety_critic(action)\n\n        # Check position constraints\n        position_safe = self.position_constraint(action)\n\n        # Check velocity constraints\n        velocity_safe = self.velocity_constraint(action)\n\n        # Combine safety scores\n        overall_safety = (safety_score + position_safe + velocity_safe) / 3\n\n        # Determine if action is safe\n        is_safe = overall_safety > self.safety_threshold\n\n        return {\n            'is_safe': is_safe,\n            'safety_score': overall_safety,\n            'breakdown': {\n                'safety': safety_score,\n                'position': position_safe,\n                'velocity': velocity_safe\n            }\n        }\n\n    def safe_action_generation(self, unsafe_action, current_state=None):\n        \"\"\"Generate safe action from potentially unsafe one\"\"\"\n        # Clamp action to safe ranges\n        safe_action = torch.clamp(unsafe_action, -1.0, 1.0)\n\n        # Further refine using safety validation\n        validation = self.forward(safe_action, current_state)\n\n        if not validation['is_safe']:\n            # Reduce action magnitude until safe\n            scale_factor = 0.9\n            while not validation['is_safe'] and scale_factor > 0.1:\n                scaled_action = safe_action * scale_factor\n                validation = self.forward(scaled_action, current_state)\n                scale_factor -= 0.1\n\n            safe_action = scaled_action if validation['is_safe'] else torch.zeros_like(safe_action)\n\n        return safe_action, validation\n"})}),"\n",(0,a.jsx)(e.h2,{id:"exercise-implement-safe-action-execution-system",children:"Exercise: Implement Safe Action Execution System"}),"\n",(0,a.jsx)(e.p,{children:"Create a system that:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Generates actions from vision-language input"}),"\n",(0,a.jsx)(e.li,{children:"Validates actions for safety before execution"}),"\n",(0,a.jsx)(e.li,{children:"Implements hierarchical planning for complex tasks"}),"\n",(0,a.jsx)(e.li,{children:"Provides feedback on action execution success"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Action generation and execution in VLA systems represents the critical link between perception and physical interaction. The systems must not only generate appropriate actions based on multimodal input but also ensure these actions are safe, feasible, and aligned with the intended task. Hierarchical planning, safety validation, and real-time execution are essential components for creating robust and reliable robotic systems."}),"\n",(0,a.jsx)(e.hr,{})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(_,{...n})}):_(n)}},8453(n,e,t){t.d(e,{R:()=>s,x:()=>c});var i=t(6540);const a={},o=i.createContext(a);function s(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);