"use strict";(globalThis.webpackChunkphysical_humanoid_robotics_book=globalThis.webpackChunkphysical_humanoid_robotics_book||[]).push([[787],{7056(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>_,frontMatter:()=>s,metadata:()=>o,toc:()=>m});const o=JSON.parse('{"id":"modules/module-5/module-5-chapter-5","title":"Capstone Project: Complete Humanoid Robot System","description":"This capstone chapter integrates all components developed throughout the course into a complete, functional humanoid robot system. This represents the culmination of all previous modules, combining the robotic nervous system, digital twin, AI brain, and vision-language-action capabilities into a unified autonomous platform.","source":"@site/content/modules/module-5/chapter-5.md","sourceDirName":"modules/module-5","slug":"/modules/module-5/module-5-chapter-5","permalink":"/ur/docs/modules/module-5/module-5-chapter-5","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-humanoid-robotics/physical-humanoid-robotics-book/tree/main/packages/create-docusaurus/templates/shared/content/modules/module-5/chapter-5.md","tags":[],"version":"current","frontMatter":{"id":"module-5-chapter-5","title":"Capstone Project: Complete Humanoid Robot System","sidebar_label":"Capstone Project"},"sidebar":"tutorialSidebar","previous":{"title":"Integration & Testing","permalink":"/ur/docs/modules/module-5/module-5-chapter-4"},"next":{"title":"Conclusion","permalink":"/ur/docs/conclusion"}}');var a=t(4848),i=t(8453);const s={id:"module-5-chapter-5",title:"Capstone Project: Complete Humanoid Robot System",sidebar_label:"Capstone Project"},r="Capstone Project: Complete Humanoid Robot System",l={},m=[{value:"Complete System Architecture",id:"complete-system-architecture",level:2},{value:"Main System Orchestrator",id:"main-system-orchestrator",level:2},{value:"Complete Humanoid Robot Node",id:"complete-humanoid-robot-node",level:3},{value:"Performance Monitoring and Optimization",id:"performance-monitoring-and-optimization",level:2},{value:"System Performance Monitor",id:"system-performance-monitor",level:3},{value:"System Integration Testing",id:"system-integration-testing",level:2},{value:"Comprehensive Integration Test Suite",id:"comprehensive-integration-test-suite",level:3},{value:"Deployment Configuration",id:"deployment-configuration",level:2},{value:"System Deployment Scripts",id:"system-deployment-scripts",level:3},{value:"Exercise: Complete System Integration",id:"exercise-complete-system-integration",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"capstone-project-complete-humanoid-robot-system",children:"Capstone Project: Complete Humanoid Robot System"})}),"\n",(0,a.jsx)(n.p,{children:"This capstone chapter integrates all components developed throughout the course into a complete, functional humanoid robot system. This represents the culmination of all previous modules, combining the robotic nervous system, digital twin, AI brain, and vision-language-action capabilities into a unified autonomous platform."}),"\n",(0,a.jsx)(n.h2,{id:"complete-system-architecture",children:"Complete System Architecture"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    COMPLETE HUMANOID ROBOT SYSTEM                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  PERCEPTION LAYER                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Vision        \u2502   Audio         \u2502   Tactile       \u2502   Environmental      \u2502 \u2502\n\u2502  \u2502   Processing    \u2502   Processing    \u2502   Sensors       \u2502   Monitoring       \u2502 \u2502\n\u2502  \u2502   \u2022 Cameras     \u2502   \u2022 Microphones \u2502   \u2022 Force/Torque\u2502   \u2022 IMU            \u2502 \u2502\n\u2502  \u2502   \u2022 LIDAR       \u2502   \u2022 Speakers    \u2502   \u2022 Joint Encoders\u2502 \u2022 Pressure       \u2502 \u2502\n\u2502  \u2502   \u2022 Depth       \u2502   \u2022 Echo Cancel \u2502   \u2022 Temperature \u2502   \u2022 Temperature    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                                 \u2502\n\u2502  AI PROCESSING LAYER                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  \u2022 Computer Vision    \u2022 NLP Processing    \u2022 Motion Planning               \u2502 \u2502\n\u2502  \u2502  \u2022 SLAM               \u2022 Speech Synthesis  \u2022 Path Planning                 \u2502 \u2502\n\u2502  \u2502  \u2022 Object Detection   \u2022 Dialog Manager    \u2022 Trajectory Generation         \u2502 \u2502\n\u2502  \u2502  \u2022 Semantic Mapping   \u2022 Command Parser    \u2022 Balance Control               \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                                 \u2502\n\u2502  CONTROL LAYER                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Locomotion    \u2502   Manipulation  \u2502   Balance       \u2502   Whole-Body       \u2502 \u2502\n\u2502  \u2502   Control       \u2502   Control       \u2502   Control       \u2502   Coordination     \u2502 \u2502\n\u2502  \u2502   \u2022 Walking     \u2502   \u2022 Grasping    \u2502   \u2022 Posture     \u2502   \u2022 Joint Control  \u2502 \u2502\n\u2502  \u2502   \u2022 Turning     \u2502   \u2022 Reaching    \u2502   \u2022 Stability   \u2502   \u2022 Trajectory     \u2502 \u2502\n\u2502  \u2502   \u2022 Stair Climbing\u2502 \u2022 Manipulation\u2502   \u2022 Recovery    \u2502   \u2022 Safety Limits  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                                 \u2502\n\u2502  ORCHESTRATION LAYER                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  \u2022 Behavior Manager \u2502  \u2022 Task Planner   \u2502  \u2022 State Machine              \u2502 \u2502\n\u2502  \u2502  \u2022 Skill Library    \u2502  \u2022 Motion Planner \u2502  \u2022 Safety Validator           \u2502 \u2502\n\u2502  \u2502  \u2022 Human-Robot     \u2502  \u2022 Action Executor\u2502  \u2022 Performance Monitor        \u2502 \u2502\n\u2502  \u2502  \u2022 Interaction     \u2502                   \u2502                                \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                                 \u2502\n\u2502  ROS 2 COMMUNICATION LAYER                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  \u2022 Node Management   \u2022 Topic Communication   \u2022 Service Interfaces         \u2502 \u2502\n\u2502  \u2502  \u2022 Action Servers    \u2022 Parameter Server      \u2022 TF Transformations         \u2502 \u2502\n\u2502  \u2502  \u2022 Lifecycle Nodes   \u2022 Bag Recording       \u2022 Diagnostic Tools           \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                                 \u2502\n\u2502  HARDWARE ABSTRACTION LAYER                                                    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Joint Drivers \u2502   Sensor        \u2502   Power         \u2502   Communication      \u2502 \u2502\n\u2502  \u2502   \u2022 Servo Ctrl  \u2502   \u2022 ADC/DAC     \u2502   \u2022 Battery     \u2502   \u2022 Ethernet       \u2502 \u2502\n\u2502  \u2502   \u2022 Motor Ctrl  \u2502   \u2022 IMU         \u2502   \u2022 Power Dist  \u2502   \u2022 WiFi           \u2502 \u2502\n\u2502  \u2502   \u2022 PID Tuning  \u2502   \u2022 Encoders    \u2502   \u2022 DC-DC       \u2502   \u2022 CAN Bus        \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,a.jsx)(n.h2,{id:"main-system-orchestrator",children:"Main System Orchestrator"}),"\n",(0,a.jsx)(n.h3,{id:"complete-humanoid-robot-node",children:"Complete Humanoid Robot Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Image, Imu, PointCloud2\nfrom geometry_msgs.msg import Twist, Pose\nfrom std_msgs.msg import String, Bool\nfrom nav_msgs.msg import Odometry\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\nimport threading\nimport time\nfrom typing import Dict, Any, Optional\nimport json\n\nclass HumanoidOrchestrator(Node):\n    def __init__(self):\n        super().__init__('humanoid_orchestrator')\n\n        # Initialize subsystem managers\n        self.perception_manager = PerceptionManager(self)\n        self.ai_brain = AIBrain(self)\n        self.control_manager = ControlManager(self)\n        self.communication_manager = CommunicationManager(self)\n        self.safety_validator = SafetyValidator(self)\n\n        # Robot state\n        self.robot_state = {\n            'joint_states': {},\n            'sensor_data': {},\n            'location': [0.0, 0.0, 0.0],\n            'orientation': [0.0, 0.0, 0.0, 1.0],  # quaternion\n            'battery_level': 100.0,\n            'temperature': 25.0,\n            'operational_mode': 'idle',\n            'safety_status': 'nominal'\n        }\n\n        # Task management\n        self.task_queue = queue.Queue()\n        self.active_task = None\n        self.task_lock = threading.Lock()\n\n        # ROS 2 interfaces\n        self.joint_state_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_state_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )\n        self.camera_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.camera_callback, 10\n        )\n        self.lidar_sub = self.create_subscription(\n            PointCloud2, '/lidar/points', self.lidar_callback, 10\n        )\n        self.odom_sub = self.create_subscription(\n            Odometry, '/odom', self.odom_callback, 10\n        )\n\n        # Command interfaces\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.behavior_cmd_pub = self.create_publisher(String, '/behavior_command', 10)\n        self.status_pub = self.create_publisher(String, '/system_status', 10)\n        self.action_cmd_sub = self.create_subscription(\n            String, '/action_command', self.action_command_callback, 10\n        )\n\n        # System control\n        self.system_initialized = False\n        self.operational_mode = 'idle'  # idle, autonomous, teleoperation, maintenance\n        self.emergency_stop = False\n\n        # Performance monitoring\n        self.performance_metrics = {\n            'cpu_usage': 0.0,\n            'memory_usage': 0.0,\n            'control_loop_rate': 0.0,\n            'sensor_data_rate': 0.0\n        }\n\n        # Initialize system\n        self.initialize_system()\n\n        # Start main control loop\n        self.main_loop_timer = self.create_timer(0.01, self.main_control_loop)  # 100Hz\n\n    def initialize_system(self):\n        \"\"\"Initialize complete humanoid system\"\"\"\n        self.get_logger().info(\"Initializing Humanoid Robot System...\")\n\n        # Initialize perception system\n        self.perception_manager.initialize()\n\n        # Initialize AI brain\n        self.ai_brain.initialize()\n\n        # Initialize control system\n        self.control_manager.initialize()\n\n        # Initialize communication manager\n        self.communication_manager.initialize()\n\n        # Initialize safety validator\n        self.safety_validator.initialize()\n\n        # Wait for all systems to be ready\n        time.sleep(2.0)\n\n        # Set operational mode\n        self.operational_mode = 'idle'\n        self.system_initialized = True\n\n        self.get_logger().info(\"Humanoid Robot System Initialization Complete!\")\n\n    def joint_state_callback(self, msg):\n        \"\"\"Update joint state information\"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.robot_state['joint_states'][name] = {\n                    'position': msg.position[i],\n                    'velocity': msg.velocity[i] if i < len(msg.velocity) else 0.0,\n                    'effort': msg.effort[i] if i < len(msg.effort) else 0.0\n                }\n\n    def imu_callback(self, msg):\n        \"\"\"Update IMU data\"\"\"\n        self.robot_state['sensor_data']['imu'] = {\n            'orientation': [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w],\n            'angular_velocity': [msg.angular_velocity.x, msg.angular_velocity.y, msg.angular_velocity.z],\n            'linear_acceleration': [msg.linear_acceleration.x, msg.linear_acceleration.y, msg.linear_acceleration.z]\n        }\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera data\"\"\"\n        # Forward to perception system\n        self.perception_manager.process_camera_data(msg)\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LIDAR data\"\"\"\n        # Forward to perception system\n        self.perception_manager.process_lidar_data(msg)\n\n    def odom_callback(self, msg):\n        \"\"\"Update odometry information\"\"\"\n        self.robot_state['location'] = [\n            msg.pose.pose.position.x,\n            msg.pose.pose.y,\n            msg.pose.pose.position.z\n        ]\n        self.robot_state['orientation'] = [\n            msg.pose.pose.orientation.x,\n            msg.pose.pose.y,\n            msg.pose.pose.orientation.z,\n            msg.pose.pose.orientation.w\n        ]\n\n    def action_command_callback(self, msg):\n        \"\"\"Process action commands\"\"\"\n        try:\n            command_data = json.loads(msg.data)\n            command_type = command_data.get('type', 'unknown')\n\n            if self.emergency_stop:\n                self.get_logger().warn(\"Emergency stop active - ignoring command\")\n                return\n\n            # Validate command safety\n            if self.safety_validator.validate_command(command_data, self.robot_state):\n                # Add to task queue\n                self.task_queue.put({\n                    'type': command_type,\n                    'data': command_data,\n                    'timestamp': time.time()\n                })\n            else:\n                self.get_logger().error(f\"Safety validation failed for command: {command_type}\")\n\n        except json.JSONDecodeError:\n            self.get_logger().error(f'Invalid JSON command: {msg.data}')\n\n    def main_control_loop(self):\n        \"\"\"Main system control loop running at 100Hz\"\"\"\n        if not self.system_initialized:\n            return\n\n        loop_start_time = time.time()\n\n        # Process perception data\n        perception_data = self.perception_manager.get_current_perception()\n\n        # Get AI decision\n        ai_decision = self.ai_brain.make_decision(\n            perception_data=perception_data,\n            robot_state=self.robot_state\n        )\n\n        # Generate control commands\n        control_commands = self.control_manager.generate_commands(ai_decision)\n\n        # Execute commands if safe\n        if self.safety_validator.validate_action(control_commands, self.robot_state):\n            self.control_manager.execute_commands(control_commands)\n        else:\n            self.get_logger().warn(\"Control command rejected by safety validator\")\n\n        # Process tasks from queue\n        self.process_task_queue()\n\n        # Update performance metrics\n        self.update_performance_metrics()\n\n        # Publish system status\n        self.publish_system_status()\n\n        # Calculate loop timing\n        loop_time = time.time() - loop_start_time\n        self.performance_metrics['control_rate'] = 1.0 / loop_time if loop_time > 0 else 0\n\n    def process_task_queue(self):\n        \"\"\"Process queued tasks\"\"\"\n        if not self.task_queue.empty():\n            try:\n                task = self.task_queue.get_nowait()\n\n                with self.task_lock:\n                    self.active_task = task\n\n                # Execute task based on type\n                if task['type'] == 'navigate':\n                    self.execute_navigation_task(task)\n                elif task['type'] == 'manipulate':\n                    self.execute_manipulation_task(task)\n                elif task['type'] == 'interact':\n                    self.execute_interaction_task(task)\n                elif task['type'] == 'change_mode':\n                    self.change_operational_mode(task['data'].get('mode', 'idle'))\n                else:\n                    self.get_logger().warn(f'Unknown task type: {task[\"type\"]}')\n\n                # Mark task as complete\n                with self.task_lock:\n                    self.active_task = None\n\n            except queue.Empty:\n                pass  # Queue was empty\n\n    def execute_navigation_task(self, task):\n        \"\"\"Execute navigation task\"\"\"\n        destination = task['data'].get('destination', [0.0, 0.0, 0.0])\n        mode = task['data'].get('mode', 'safe')\n\n        # Plan navigation\n        navigation_plan = self.ai_brain.plan_navigation(\n            start_pos=self.robot_state['location'],\n            destination=destination,\n            mode=mode\n        )\n\n        # Execute navigation\n        self.control_manager.execute_navigation(navigation_plan)\n\n    def execute_manipulation_task(self, task):\n        \"\"\"Execute manipulation task\"\"\"\n        target_object = task['data'].get('object', '')\n        action = task['data'].get('action', 'grasp')\n\n        # Plan manipulation\n        manipulation_plan = self.ai_brain.plan_manipulation(\n            target_object=target_object,\n            action=action\n        )\n\n        # Execute manipulation\n        self.control_manager.execute_manipulation(manipulation_plan)\n\n    def execute_interaction_task(self, task):\n        \"\"\"Execute interaction task\"\"\"\n        target = task['data'].get('target', 'person')\n        interaction_type = task['data'].get('type', 'greet')\n\n        # Plan interaction\n        interaction_plan = self.ai_brain.plan_interaction(\n            target=target,\n            interaction_type=interaction_type\n        )\n\n        # Execute interaction\n        self.control_manager.execute_interaction(interaction_plan)\n\n    def change_operational_mode(self, new_mode):\n        \"\"\"Change operational mode\"\"\"\n        valid_modes = ['idle', 'autonomous', 'teleoperation', 'maintenance', 'emergency_stop']\n\n        if new_mode in valid_modes:\n            old_mode = self.operational_mode\n            self.operational_mode = new_mode\n\n            self.get_logger().info(f'Operational mode changed from {old_mode} to {new_mode}')\n\n            # Execute mode-specific actions\n            if new_mode == 'emergency_stop':\n                self.emergency_stop = True\n                self.control_manager.emergency_stop()\n            elif new_mode == 'autonomous':\n                self.emergency_stop = False\n                self.enable_autonomous_mode()\n            elif new_mode == 'teleoperation':\n                self.emergency_stop = False\n                self.enable_teleoperation_mode()\n            elif new_mode == 'maintenance':\n                self.emergency_stop = False\n                self.enable_maintenance_mode()\n            elif new_mode == 'idle':\n                self.emergency_stop = False\n                self.enable_idle_mode()\n        else:\n            self.get_logger().warn(f'Invalid operational mode: {new_mode}')\n\n    def enable_autonomous_mode(self):\n        \"\"\"Enable autonomous operation mode\"\"\"\n        # Activate perception system\n        self.perception_manager.activate_autonomous_mode()\n\n        # Enable AI decision making\n        self.ai_brain.enable_decision_making()\n\n        # Set control system to autonomous\n        self.control_manager.set_autonomous_mode()\n\n    def enable_teleoperation_mode(self):\n        \"\"\"Enable teleoperation mode\"\"\"\n        # Deactivate autonomous perception\n        self.perception_manager.deactivate_autonomous_mode()\n\n        # Disable AI decision making\n        self.ai_brain.disable_decision_making()\n\n        # Set control system to manual\n        self.control_manager.set_manual_mode()\n\n    def enable_maintenance_mode(self):\n        \"\"\"Enable maintenance mode\"\"\"\n        # Disable all autonomous functions\n        self.perception_manager.deactivate_all_modes()\n        self.ai_brain.disable_all_ai_functions()\n        self.control_manager.set_maintenance_mode()\n\n    def enable_idle_mode(self):\n        \"\"\"Enable idle mode\"\"\"\n        # Keep basic monitoring active\n        self.perception_manager.activate_monitoring_mode()\n        self.ai_brain.enable_basic_monitoring()\n        self.control_manager.set_idle_mode()\n\n    def update_performance_metrics(self):\n        \"\"\"Update system performance metrics\"\"\"\n        # This would monitor actual system performance\n        # For now, using placeholder values\n        import psutil\n        try:\n            import GPUtil\n            gpus = GPUtil.getGPUs()\n            if gpus:\n                self.performance_metrics['gpu_usage'] = gpus[0].load * 100\n        except:\n            pass\n\n        self.performance_metrics['cpu_usage'] = psutil.cpu_percent()\n        self.performance_metrics['memory_usage'] = psutil.virtual_memory().percent\n\n    def publish_system_status(self):\n        \"\"\"Publish system status\"\"\"\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'operational_mode': self.operational_mode,\n            'location': self.robot_state['location'],\n            'battery_level': self.robot_state['battery_level'],\n            'temperature': self.robot_state['temperature'],\n            'performance_metrics': self.performance_metrics,\n            'active_task': self.active_task['type'] if self.active_task else None,\n            'safety_status': self.robot_state['safety_status'],\n            'timestamp': time.time()\n        })\n\n        self.status_pub.publish(status_msg)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"performance-monitoring-and-optimization",children:"Performance Monitoring and Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"system-performance-monitor",children:"System Performance Monitor"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import psutil\nimport GPUtil\nimport time\nfrom collections import deque\nimport threading\n\nclass SystemPerformanceMonitor:\n    def __init__(self, robot_node):\n        self.robot_node = robot_node\n        self.performance_data = {\n            'cpu_usage': deque(maxlen=100),\n            'memory_usage': deque(maxlen=100),\n            'gpu_usage': deque(maxlen=100),\n            'control_loop_times': deque(maxlen=100),\n            'sensor_processing_times': deque(maxlen=100),\n            'ai_processing_times': deque(maxlen=100)\n        }\n\n        self.monitoring_active = True\n        self.monitoring_thread = threading.Thread(target=self.monitor_loop, daemon=True)\n        self.monitoring_thread.start()\n\n    def monitor_loop(self):\n        \"\"\"Continuous monitoring loop\"\"\"\n        while self.monitoring_active and rclpy.ok():\n            # Monitor system resources\n            self.performance_data['cpu_usage'].append(psutil.cpu_percent())\n            self.performance_data['memory_usage'].append(psutil.virtual_memory().percent)\n\n            # Monitor GPU if available\n            gpus = GPUtil.getGPUs()\n            if gpus:\n                self.performance_data['gpu_usage'].append(gpus[0].load * 100)\n\n            time.sleep(0.1)  # Monitor every 100ms\n\n    def get_performance_summary(self):\n        \"\"\"Get performance summary\"\"\"\n        summary = {}\n        for key, data in self.performance_data.items():\n            if data:\n                summary[key] = {\n                    'current': data[-1] if data else 0,\n                    'average': sum(data) / len(data),\n                    'peak': max(data) if data else 0\n                }\n            else:\n                summary[key] = {'current': 0, 'average': 0, 'peak': 0}\n\n        return summary\n\n    def check_performance_thresholds(self):\n        \"\"\"Check if performance is within acceptable thresholds\"\"\"\n        issues = []\n\n        # Check CPU usage\n        if self.performance_data['cpu_usage'] and self.get_average('cpu_usage') > 90:\n            issues.append('High CPU usage')\n\n        # Check memory usage\n        if self.performance_data['memory_usage'] and self.get_average('memory_usage') > 90:\n            issues.append('High memory usage')\n\n        # Check GPU usage\n        if self.performance_data['gpu_usage'] and self.get_average('gpu_usage') > 95:\n            issues.append('High GPU usage')\n\n        return issues\n\n    def get_average(self, metric_name):\n        \"\"\"Get average value for a metric\"\"\"\n        data = self.performance_data[metric_name]\n        return sum(data) / len(data) if data else 0\n\nclass SafetyValidator:\n    def __init__(self, robot_node):\n        self.robot_node = robot_node\n        self.safety_limits = {\n            'max_joint_velocity': 5.0,  # rad/s\n            'max_joint_torque': 100.0,  # Nm\n            'max_linear_velocity': 1.0,  # m/s\n            'max_angular_velocity': 1.0,  # rad/s\n            'min_battery_level': 10.0,  # %\n            'max_temperature': 70.0  # Celsius\n        }\n\n        self.safety_violations = []\n        self.emergency_stop_active = False\n\n    def validate_command(self, command, robot_state):\n        \"\"\"Validate command for safety\"\"\"\n        violations = []\n\n        # Check joint limits\n        if 'joint_commands' in command:\n            for joint_name, cmd in command['joint_commands'].items():\n                if joint_name in robot_state['joint_states']:\n                    current_pos = robot_state['joint_states'][joint_name]['position']\n                    target_pos = cmd.get('position', current_pos)\n\n                    # Check velocity limits\n                    if 'velocity' in cmd and abs(cmd['velocity']) > self.safety_limits['max_joint_velocity']:\n                        violations.append(f'Joint {joint_name} velocity limit exceeded')\n\n                    # Check torque limits\n                    if 'effort' in cmd and abs(cmd['effort']) > self.safety_limits['max_joint_torque']:\n                        violations.append(f'Joint {joint_name} torque limit exceeded')\n\n        # Check motion limits\n        if 'motion_commands' in command:\n            linear_vel = command['motion_commands'].get('linear_velocity', 0)\n            angular_vel = command['motion_commands'].get('angular_velocity', 0)\n\n            if abs(linear_vel) > self.safety_limits['max_linear_velocity']:\n                violations.append('Linear velocity limit exceeded')\n\n            if abs(angular_vel) > self.safety_limits['max_angular_velocity']:\n                violations.append('Angular velocity limit exceeded')\n\n        # Check system status\n        if robot_state['battery_level'] < self.safety_limits['min_battery_level']:\n            violations.append('Battery level too low')\n\n        if robot_state['temperature'] > self.safety_limits['max_temperature']:\n            violations.append('Temperature too high')\n\n        # Log violations\n        if violations:\n            self.safety_violations.extend(violations)\n            self.robot_node.get_logger().warn(f'Safety violations: {violations}')\n\n        return len(violations) == 0\n\n    def trigger_emergency_stop(self):\n        \"\"\"Trigger emergency stop\"\"\"\n        self.emergency_stop_active = True\n        self.robot_node.get_logger().error(\"EMERGENCY STOP ACTIVATED\")\n\n    def reset_emergency_stop(self):\n        \"\"\"Reset emergency stop\"\"\"\n        self.emergency_stop_active = False\n        self.robot_node.get_logger().info(\"Emergency stop reset\")\n"})}),"\n",(0,a.jsx)(n.h2,{id:"system-integration-testing",children:"System Integration Testing"}),"\n",(0,a.jsx)(n.h3,{id:"comprehensive-integration-test-suite",children:"Comprehensive Integration Test Suite"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import unittest\nimport numpy as np\nimport time\n\nclass HumanoidSystemIntegrationTest(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up complete humanoid system for integration testing\"\"\"\n        # This would initialize the full system\n        # For now, using placeholder\n        self.system = HumanoidOrchestrator(None)  # Would need proper node\n        self.system_initialized = True\n\n    def test_perception_action_loop(self):\n        \"\"\"Test perception-action loop functionality\"\"\"\n        if not self.system_initialized:\n            self.skipTest(\"System not initialized\")\n\n        # Simulate perception input\n        perception_data = {\n            'objects': [{'class': 'person', 'position': [1.0, 0.0, 0.0], 'confidence': 0.9}],\n            'scene_description': 'Room with one person at 1 meter distance',\n            'spatial_map': {'free_space': [], 'obstacles': []}\n        }\n\n        robot_state = {\n            'location': [0.0, 0.0, 0.0],\n            'orientation': [0.0, 0.0, 0.0, 1.0],\n            'joint_states': {}\n        }\n\n        # Test AI decision making\n        ai_decision = self.system.ai_brain.make_decision(perception_data, robot_state)\n        self.assertIsNotNone(ai_decision)\n        self.assertIn('action', ai_decision)\n\n        # Test control command generation\n        control_commands = self.system.control_manager.generate_commands(ai_decision)\n        self.assertIsNotNone(control_commands)\n\n        print(\"\u2713 Perception-action loop test passed\")\n\n    def test_navigation_functionality(self):\n        \"\"\"Test navigation functionality\"\"\"\n        if not self.system_initialized:\n            self.skipTest(\"System not initialized\")\n\n        # Define navigation task\n        navigation_task = {\n            'type': 'navigate',\n            'destination': [1.0, 0.0, 0.0],\n            'mode': 'safe'\n        }\n\n        # Test navigation planning\n        navigation_plan = self.system.ai_brain.plan_navigation(\n            start_pos=[0.0, 0.0, 0.0],\n            destination=[1.0, 0.0, 0.0],\n            mode='safe'\n        )\n\n        self.assertIsNotNone(navigation_plan)\n        self.assertIn('path', navigation_plan)\n        self.assertGreater(len(navigation_plan['path']), 0)\n\n        print(\"\u2713 Navigation functionality test passed\")\n\n    def test_balance_control(self):\n        \"\"\"Test balance control system\"\"\"\n        if not self.system_initialized:\n            self.skipTest(\"System not initialized\")\n\n        # Simulate IMU data indicating imbalance\n        imu_data = {\n            'orientation': [0.1, 0.1, 0.0, 0.99],  # Slightly tilted\n            'angular_velocity': [0.05, 0.05, 0.0],\n            'linear_acceleration': [0.1, 0.1, 9.7]\n        }\n\n        # Test balance correction\n        balance_correction = self.system.balance_controller.compute_balance_correction(imu_data)\n        self.assertIsNotNone(balance_correction)\n\n        # Verify correction values are reasonable\n        self.assertTrue(all(abs(val) < 1.0 for val in balance_correction))\n\n        print(\"\u2713 Balance control test passed\")\n\n    def test_safety_validation(self):\n        \"\"\"Test safety validation system\"\"\"\n        if not self.system_initialized:\n            self.skipTest(\"System not initialized\")\n\n        # Test safe command\n        safe_command = {\n            'joints': {\n                'left_hip_pitch': {'position': 0.1, 'velocity': 0.5},\n                'right_hip_pitch': {'position': 0.1, 'velocity': 0.5}\n            }\n        }\n\n        robot_state = {\n            'joint_states': {\n                'left_hip_pitch': {'position': 0.0, 'velocity': 0.0},\n                'right_hip_pitch': {'position': 0.0, 'velocity': 0.0}\n            }\n        }\n\n        is_safe = self.system.safety_validator.validate_command(safe_command, robot_state)\n        self.assertTrue(is_safe)\n\n        # Test unsafe command (joint position beyond limits)\n        unsafe_command = {\n            'joints': {\n                'left_hip_pitch': {'position': 10.0, 'velocity': 0.5}  # Beyond joint limits\n            }\n        }\n\n        is_safe_unsafe = self.system.safety_validator.validate_command(unsafe_command, robot_state)\n        self.assertFalse(is_safe_unsafe)\n\n        print(\"\u2713 Safety validation test passed\")\n\n    def test_autonomous_behavior_sequence(self):\n        \"\"\"Test sequence of autonomous behaviors\"\"\"\n        if not self.system_initialized:\n            self.skipTest(\"System not initialized\")\n\n        # Define behavior sequence\n        behavior_sequence = [\n            {'type': 'navigate', 'target': [1.0, 0.0, 0.0]},\n            {'type': 'greet', 'target_type': 'greeting_interaction'},\n            {'type': 'navigate', 'target': [0.0, 0.0, 0.0]},\n        ]\n\n        print(\"Testing autonomous behavior sequence...\")\n\n        for i, behavior in enumerate(behavior_sequence):\n            print(f\"  Executing behavior {i+1}: {behavior['type']}\")\n\n            # Validate behavior\n            is_safe = self.system.safety_validator.validate_action(behavior, {})\n            self.assertTrue(is_safe, f\"Behavior {i+1} failed safety validation\")\n\n            # Generate commands\n            commands = self.system.control_manager.generate_commands(behavior)\n            self.assertIsNotNone(commands)\n\n        print(\"\u2713 Autonomous behavior sequence test passed\")\n\n    def test_system_performance(self):\n        \"\"\"Test system performance under load\"\"\"\n        if not self.system_initialized:\n            self.skipTest(\"System not initialized\")\n\n        # Measure performance over multiple cycles\n        start_time = time.time()\n        iterations = 100\n\n        for i in range(iterations):\n            # Simulate perception data\n            perception_data = {\n                'objects': [{'class': 'object', 'position': [i/100, 0, 0], 'confidence': 0.9}],\n                'scene_description': 'Test scene',\n                'spatial_map': {'free_space': [], 'obstacles': []}\n            }\n\n            robot_state = {\n                'location': [0.0, 0.0, 0.0],\n                'orientation': [0.0, 0.0, 0.0, 1.0],\n                'joint_states': {}\n            }\n\n            # Process through system\n            ai_decision = self.system.ai_brain.make_decision(perception_data, robot_state)\n            control_commands = self.system.control_manager.generate_commands(ai_decision)\n\n        end_time = time.time()\n        avg_time = (end_time - start_time) / iterations\n\n        # Verify system can maintain real-time performance (100Hz)\n        self.assertLess(avg_time, 0.015, f\"System too slow: {avg_time}s per iteration\")\n\n        print(f\"\u2713 System performance test passed (avg: {avg_time*1000:.1f}ms)\")\n\n    def tearDown(self):\n        \"\"\"Clean up after tests\"\"\"\n        print(\"\\nIntegration tests completed successfully!\")\n        print(\"All systems are functioning properly.\")\n\ndef run_integration_tests():\n    \"\"\"Run the complete integration test suite\"\"\"\n    print(\"Starting Humanoid Robot System Integration Tests...\\n\")\n\n    # Create test suite\n    suite = unittest.TestSuite()\n    suite.addTest(HumanoidSystemIntegrationTest('test_perception_action_loop'))\n    suite.addTest(HumanoidSystemIntegrationTest('test_navigation_functionality'))\n    suite.addTest(HumanoidSystemIntegrationTest('test_balance_control'))\n    suite.addTest(HumanoidSystemIntegrationTest('test_safety_validation'))\n    suite.addTest(HumanoidSystemIntegrationTest('test_autonomous_behavior_sequence'))\n    suite.addTest(HumanoidSystemIntegrationTest('test_system_performance'))\n\n    # Run tests\n    runner = unittest.TextTestRunner(verbosity=2)\n    result = runner.run(suite)\n\n    # Generate summary\n    print(f\"\\n{'='*50}\")\n    print(\"INTEGRATION TEST SUMMARY\")\n    print(f\"{'='*50}\")\n    print(f\"Tests run: {result.testsRun}\")\n    print(f\"Failures: {len(result.failures)}\")\n    print(f\"Errors: {len(result.errors)}\")\n\n    if result.failures:\n        print(\"\\nFAILURES:\")\n        for test, traceback in result.failures:\n            print(f\"  {test}: {traceback}\")\n\n    if result.errors:\n        print(\"\\nERRORS:\")\n        for test, traceback in result.errors:\n            print(f\"  {test}: {traceback}\")\n\n    if result.wasSuccessful():\n        print(\"\\n\ud83c\udf89 ALL INTEGRATION TESTS PASSED!\")\n        print(\"The humanoid robot system is ready for deployment.\")\n    else:\n        print(\"\\n\u274c Some tests failed. Please address the issues before deployment.\")\n\n    return result.wasSuccessful()\n\nif __name__ == '__main__':\n    success = run_integration_tests()\n    exit(0 if success else 1)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"deployment-configuration",children:"Deployment Configuration"}),"\n",(0,a.jsx)(n.h3,{id:"system-deployment-scripts",children:"System Deployment Scripts"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\nimport subprocess\nimport yaml\nfrom pathlib import Path\n\nclass SystemDeployer:\n    def __init__(self):\n        self.config = self.load_configuration()\n        self.deployment_path = Path(self.config.get('deployment_path', './deployment'))\n\n    def load_configuration(self):\n        \"\"\"Load system configuration\"\"\"\n        config_path = Path('./config/system_config.yaml')\n\n        if config_path.exists():\n            with open(config_path, 'r') as f:\n                return yaml.safe_load(f)\n        else:\n            # Default configuration\n            return {\n                'deployment_path': './deployment',\n                'robot_name': 'humanoid_robot',\n                'control_frequency': 100,\n                'safety_limits': {\n                    'max_linear_velocity': 1.0,\n                    'max_angular_velocity': 1.0,\n                    'max_joint_velocity': 5.0,\n                    'max_torque': 100.0\n                },\n                'communication': {\n                    'ros_domain_id': 0,\n                    'network_interface': 'eth0'\n                }\n            }\n\n    def create_deployment_package(self):\n        \"\"\"Create deployment package with all necessary files\"\"\"\n        # Create deployment directory structure\n        self.deployment_path.mkdir(parents=True, exist_ok=True)\n\n        # Create configuration files\n        self.create_robot_config()\n        self.create_controller_config()\n        self.create_launch_files()\n\n        # Create documentation\n        self.create_deployment_documentation()\n\n        print(f\"Deployment package created at: {self.deployment_path}\")\n\n    def create_robot_config(self):\n        \"\"\"Create robot configuration files\"\"\"\n        robot_config = {\n            'robot_description': 'humanoid_robot_description',\n            'joint_limits': {\n                'left_hip_yaw': {'min': -1.5, 'max': 1.5, 'velocity': 2.0},\n                'left_hip_roll': {'min': -0.5, 'max': 0.5, 'velocity': 2.0},\n                'left_hip_pitch': {'min': -2.0, 'max': 0.5, 'velocity': 2.0},\n                # Add all other joints...\n            },\n            'sensors': {\n                'imu': {'rate': 100, 'type': 'vectornav'},\n                'cameras': [\n                    {'name': 'head_camera', 'resolution': [640, 480], 'fov': 60},\n                    {'name': 'stereo_camera', 'resolution': [1280, 720], 'fov': 90}\n                ],\n                'lidar': {'range': 10.0, 'resolution': 0.5}\n            }\n        }\n\n        config_path = self.deployment_path / 'config' / 'robot.yaml'\n        config_path.parent.mkdir(exist_ok=True)\n\n        with open(config_path, 'w') as f:\n            yaml.dump(robot_config, f, default_flow_style=False)\n\n    def create_controller_config(self):\n        \"\"\"Create controller configuration files\"\"\"\n        controller_config = {\n            'controller_manager': {\n                'ros__parameters': {\n                    'update_rate': 100,\n                    'use_sim_time': False\n                }\n            },\n            'joint_trajectory_controller': {\n                'ros__parameters': {\n                    'joints': [\n                        'left_hip_yaw', 'left_hip_roll', 'left_hip_pitch',\n                        'left_knee', 'left_ankle_pitch', 'left_ankle_roll',\n                        'right_hip_yaw', 'right_hip_roll', 'right_hip_pitch',\n                        'right_knee', 'right_ankle_pitch', 'right_ankle_roll',\n                        'left_shoulder_pitch', 'left_shoulder_roll', 'left_elbow',\n                        'right_shoulder_pitch', 'right_shoulder_roll', 'right_elbow',\n                        'neck_yaw', 'neck_pitch'\n                    ],\n                    'command_interfaces': ['position'],\n                    'state_interfaces': ['position', 'velocity'],\n                    'constraints': {\n                        'stopped_velocity_tolerance': 0.01,\n                        'goal_time': 0.5\n                    }\n                }\n            }\n        }\n\n        config_path = self.deployment_path / 'config' / 'controllers.yaml'\n        config_path.parent.mkdir(exist_ok=True)\n\n        with open(config_path, 'w') as f:\n            yaml.dump(controller_config, f, default_flow_style=False)\n\n    def create_launch_files(self):\n        \"\"\"Create launch files for the system\"\"\"\n        launch_content = '''<?xml version=\"1.0\"?>\n<launch>\n  \x3c!-- Robot State Publisher --\x3e\n  <node pkg=\"robot_state_publisher\" exec=\"robot_state_publisher\" name=\"robot_state_publisher\">\n    <param name=\"robot_description\" value=\"$(find-pkg-share my_humanoid_robot_description)/urdf/humanoid.urdf\"/>\n  </node>\n\n  \x3c!-- Joint State Publisher --\x3e\n  <node pkg=\"joint_state_publisher\" exec=\"joint_state_publisher\" name=\"joint_state_publisher\">\n    <param name=\"use_gui\" value=\"false\"/>\n  </node>\n\n  \x3c!-- Controller Manager --\x3e\n  <node pkg=\"controller_manager\" exec=\"ros2_control_node\" name=\"controller_manager\">\n    <param name=\"robot_description\" value=\"$(find-pkg-share my_humanoid_robot_description)/urdf/humanoid.urdf\"/>\n    <remap from=\"/joint_states\" to=\"dynamic_joint_states\"/>\n  </node>\n\n  \x3c!-- Start controllers --\x3e\n  <node pkg=\"controller_manager\" exec=\"spawner\" name=\"joint_state_broadcaster_spawner\" args=\"joint_state_broadcaster\"/>\n  <node pkg=\"controller_manager\" exec=\"spawner\" name=\"joint_trajectory_controller_spawner\" args=\"joint_trajectory_controller\"/>\n\n  \x3c!-- Main orchestrator --\x3e\n  <node pkg=\"humanoid_system\" exec=\"humanoid_orchestrator\" name=\"humanoid_orchestrator\" output=\"screen\">\n    <param name=\"control_frequency\" value=\"100\"/>\n    <param name=\"safety_enabled\" value=\"true\"/>\n  </node>\n\n  \x3c!-- Perception system --\x3e\n  <node pkg=\"humanoid_perception\" exec=\"perception_node\" name=\"perception_node\" output=\"screen\">\n    <param name=\"detection_model\" value=\"yolov8n.pt\"/>\n    <param name=\"tracking_enabled\" value=\"true\"/>\n  </node>\n\n  \x3c!-- AI brain --\x3e\n  <node pkg=\"humanoid_ai\" exec=\"ai_brain_node\" name=\"ai_brain_node\" output=\"screen\">\n    <param name=\"model_path\" value=\"$(find-pkg-share humanoid_ai)/models/vla_model.pt\"/>\n    <param name=\"enable_decision_making\" value=\"true\"/>\n  </node>\n</launch>\n'''\n\n        launch_path = self.deployment_path / 'launch' / 'humanoid_system.launch.xml'\n        launch_path.parent.mkdir(exist_ok=True)\n\n        with open(launch_path, 'w') as f:\n            f.write(launch_content)\n\n    def create_deployment_documentation(self):\n        \"\"\"Create deployment documentation\"\"\"\n        docs_content = f\"\"\"# Humanoid Robot Deployment Guide\n\n## System Overview\nThis document describes the deployment of the autonomous humanoid robot system.\n\n## Hardware Requirements\n- NVIDIA Jetson Orin AGX or equivalent\n- Real-time capable Linux system\n- RT kernel configured\n- Sufficient RAM and storage for AI models\n\n## Software Dependencies\n- ROS 2 Humble Hawksbill\n- Python 3.10+\n- CUDA 11.8+\n- NVIDIA Isaac packages\n- Required Python packages (see requirements.txt)\n\n## Deployment Steps\n1. Install system dependencies\n2. Clone the repository\n3. Build all packages: `colcon build`\n4. Source the workspace: `source install/setup.bash`\n5. Launch the system: `ros2 launch humanoid_system humanoid_system.launch.xml`\n\n## Configuration\nThe system can be configured through the configuration files in the `config/` directory:\n- `robot.yaml`: Robot-specific parameters\n- `controllers.yaml`: Controller configurations\n- `safety.yaml`: Safety limits and constraints\n\n## Troubleshooting\n- Check ROS 2 domain ID if multiple robots are on the same network\n- Verify sensor connections and calibrations\n- Monitor system resources (CPU, GPU, memory usage)\n\n## Safety Considerations\n- Ensure adequate space for robot operation\n- Keep emergency stop readily accessible\n- Supervise robot during initial operation\n- Regular safety system checks required\n\n## Support\nFor technical support, contact: team@humanoid-robotics.com\n\"\"\"\n\n        docs_path = self.deployment_path / 'README.md'\n        with open(docs_path, 'w') as f:\n            f.write(docs_content)\n\ndef deploy_system():\n    \"\"\"Deploy the complete humanoid robot system\"\"\"\n    print(\"Starting Humanoid Robot System Deployment...\")\n\n    deployer = SystemDeployer()\n    deployer.create_deployment_package()\n\n    print(\"Deployment package created successfully!\")\n    print(f\"Deployment location: {deployer.deployment_path.absolute()}\")\n\n    print(\"\\nNext steps:\")\n    print(\"1. Transfer deployment package to target system\")\n    print(\"2. Install dependencies\")\n    print(\"3. Configure hardware interfaces\")\n    print(\"4. Run integration tests\")\n    print(\"5. Begin operational deployment\")\n\nif __name__ == \"__main__\":\n    deploy_system()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"exercise-complete-system-integration",children:"Exercise: Complete System Integration"}),"\n",(0,a.jsx)(n.p,{children:"Create a complete system integration that:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Combines all five modules into a unified architecture"}),"\n",(0,a.jsx)(n.li,{children:"Implements comprehensive safety validation"}),"\n",(0,a.jsx)(n.li,{children:"Provides real-time performance monitoring"}),"\n",(0,a.jsx)(n.li,{children:"Includes automated testing and deployment scripts"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"The complete humanoid robot system represents the integration of all previous modules into a cohesive autonomous platform. The system combines multimodal perception, AI decision-making, and precise control to enable complex behaviors. Through careful design of the architecture, safety systems, and validation procedures, we create a robust platform for humanoid robotics research and applications. The system is designed for deployment on real hardware while maintaining the flexibility to adapt to various robotic platforms and applications."}),"\n",(0,a.jsx)(n.hr,{})]})}function _(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const a={},i=o.createContext(a);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);