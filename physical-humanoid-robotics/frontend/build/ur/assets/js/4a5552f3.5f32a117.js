"use strict";(globalThis.webpackChunkphysical_humanoid_robotics_book=globalThis.webpackChunkphysical_humanoid_robotics_book||[]).push([[860],{250(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"modules/module-3/module-3-chapter-2","title":"Isaac ROS Perception Pipeline","description":"This chapter explores the perception capabilities of NVIDIA Isaac ROS, focusing on how AI-powered perception systems enable robots to understand their environment.","source":"@site/content/modules/module-3/chapter-2.md","sourceDirName":"modules/module-3","slug":"/modules/module-3/module-3-chapter-2","permalink":"/ur/docs/modules/module-3/module-3-chapter-2","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-humanoid-robotics/physical-humanoid-robotics-book/tree/main/packages/create-docusaurus/templates/shared/content/modules/module-3/chapter-2.md","tags":[],"version":"current","frontMatter":{"id":"module-3-chapter-2","title":"Isaac ROS Perception Pipeline","sidebar_label":"Isaac Perception"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac AI Basics","permalink":"/ur/docs/modules/module-3/module-3-chapter-1"},"next":{"title":"Isaac Planning","permalink":"/ur/docs/modules/module-3/module-3-chapter-3"}}');var a=s(4848),t=s(8453);const o={id:"module-3-chapter-2",title:"Isaac ROS Perception Pipeline",sidebar_label:"Isaac Perception"},r="Isaac ROS Perception Pipeline",l={},c=[{value:"Isaac ROS Perception Stack",id:"isaac-ros-perception-stack",level:2},{value:"Isaac ROS Perception Nodes",id:"isaac-ros-perception-nodes",level:2},{value:"AprilTag Detection",id:"apriltag-detection",level:3},{value:"Image Segmentation",id:"image-segmentation",level:3},{value:"Isaac ROS Depth Perception",id:"isaac-ros-depth-perception",level:2},{value:"Stereo Disparity Processing",id:"stereo-disparity-processing",level:3},{value:"Isaac ROS Sensor Fusion",id:"isaac-ros-sensor-fusion",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"TensorRT Optimization",id:"tensorrt-optimization",level:3},{value:"Exercise: Build a Multi-Modal Perception System",id:"exercise-build-a-multi-modal-perception-system",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"isaac-ros-perception-pipeline",children:"Isaac ROS Perception Pipeline"})}),"\n",(0,a.jsx)(n.p,{children:"This chapter explores the perception capabilities of NVIDIA Isaac ROS, focusing on how AI-powered perception systems enable robots to understand their environment."}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-perception-stack",children:"Isaac ROS Perception Stack"}),"\n",(0,a.jsx)(n.p,{children:"The Isaac ROS perception stack includes:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visual Perception"}),": Object detection, segmentation, tracking"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth Perception"}),": Stereo vision, depth estimation, 3D reconstruction"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining multiple sensor modalities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AI Inference"}),": Optimized neural network execution"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-perception-nodes",children:"Isaac ROS Perception Nodes"}),"\n",(0,a.jsx)(n.h3,{id:"apriltag-detection",children:"AprilTag Detection"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom isaac_ros_apriltag_interfaces.msg import AprilTagDetectionArray\nimport numpy as np\n\nclass IsaacAprilTagProcessor(Node):\n    def __init__(self):\n        super().__init__(\'isaac_apriltag_processor\')\n\n        # Subscribe to camera data\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_rect_color\', self.image_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/camera/camera_info\', self.camera_info_callback, 10\n        )\n\n        # Subscribe to AprilTag detections\n        self.detection_sub = self.create_subscription(\n            AprilTagDetectionArray, \'/apriltag_detections\', self.detection_callback, 10\n        )\n\n        # Publisher for processed poses\n        self.pose_pub = self.create_publisher(PoseStamped, \'/apriltag_poses\', 10)\n\n        self.camera_intrinsics = None\n\n    def camera_info_callback(self, msg):\n        """Store camera intrinsics for pose calculation"""\n        self.camera_intrinsics = np.array(msg.k).reshape(3, 3)\n\n    def detection_callback(self, msg):\n        """Process AprilTag detections and calculate poses"""\n        for detection in msg.detections:\n            if self.camera_intrinsics is not None:\n                # Calculate pose relative to camera\n                pose = self.calculate_pose(detection, self.camera_intrinsics)\n\n                # Publish pose\n                pose_msg = PoseStamped()\n                pose_msg.header = msg.header\n                pose_msg.pose = pose\n                self.pose_pub.publish(pose_msg)\n\n    def calculate_pose(self, detection, intrinsics):\n        """Calculate pose from AprilTag detection"""\n        # Implementation would use PnP algorithm with known tag size\n        # and camera intrinsics\n        pass\n'})}),"\n",(0,a.jsx)(n.h3,{id:"image-segmentation",children:"Image Segmentation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom isaac_ros_segmentation_interfaces.msg import SegmentationTensorRT\nimport numpy as np\nimport cv2\n\nclass IsaacSegmentationProcessor(Node):\n    def __init__(self):\n        super().__init__(\'isaac_segmentation_processor\')\n\n        self.segmentation_sub = self.create_subscription(\n            SegmentationTensorRT,\n            \'/segmentation/segmentation_map\',\n            self.segmentation_callback,\n            10\n        )\n\n        self.visualization_pub = self.create_publisher(\n            Image,\n            \'/segmentation/visualization\',\n            10\n        )\n\n        # Class mapping for visualization\n        self.class_colors = {\n            0: [0, 0, 0],      # Background\n            1: [255, 0, 0],    # Person\n            2: [0, 255, 0],    # Obstacle\n            3: [0, 0, 255],    # Furniture\n            4: [255, 255, 0],  # Door\n        }\n\n    def segmentation_callback(self, msg):\n        """Process segmentation results and create visualization"""\n        # Reshape segmentation data\n        seg_map = np.array(msg.tensor.data).reshape((msg.height, msg.width))\n\n        # Create color visualization\n        vis_image = np.zeros((msg.height, msg.width, 3), dtype=np.uint8)\n\n        for class_id, color in self.class_colors.items():\n            mask = (seg_map == class_id)\n            vis_image[mask] = color\n\n        # Convert to ROS image message\n        vis_msg = self.bridge.cv2_to_imgmsg(vis_image, "bgr8")\n        vis_msg.header = msg.header\n        self.visualization_pub.publish(vis_msg)\n\n        # Process each class separately\n        self.process_persons(seg_map)\n        self.process_obstacles(seg_map)\n\n    def process_persons(self, seg_map):\n        """Process person detections from segmentation"""\n        person_mask = (seg_map == 1)\n        if np.any(person_mask):\n            # Find contours of person regions\n            contours, _ = cv2.findContours(\n                person_mask.astype(np.uint8),\n                cv2.RETR_EXTERNAL,\n                cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            for contour in contours:\n                if cv2.contourArea(contour) > 100:  # Filter small regions\n                    # Calculate bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n                    # Process person detection\n                    self.handle_person_detection(x, y, w, h)\n\n    def process_obstacles(self, seg_map):\n        """Process obstacle detections from segmentation"""\n        obstacle_mask = (seg_map == 2)\n        if np.any(obstacle_mask):\n            # Create occupancy grid or navigation costmap\n            self.update_navigation_map(obstacle_mask)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-depth-perception",children:"Isaac ROS Depth Perception"}),"\n",(0,a.jsx)(n.h3,{id:"stereo-disparity-processing",children:"Stereo Disparity Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom stereo_msgs.msg import DisparityImage\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacDepthProcessor(Node):\n    def __init__(self):\n        super().__init__(\'isaac_depth_processor\')\n\n        self.disparity_sub = self.create_subscription(\n            DisparityImage,\n            \'/stereo/disparity\',\n            self.disparity_callback,\n            10\n        )\n\n        self.depth_pub = self.create_publisher(\n            Image,\n            \'/stereo/depth\',\n            10\n        )\n\n        self.bridge = CvBridge()\n\n    def disparity_callback(self, msg):\n        """Convert disparity to depth"""\n        # Convert disparity image to numpy array\n        disparity_img = self.bridge.imgmsg_to_cv2(msg.image)\n\n        # Calculate depth from disparity\n        # depth = (baseline * focal_length) / disparity\n        baseline = msg.t.max_disparity  # This is a simplification\n        focal_length = msg.f  # From camera calibration\n\n        # Avoid division by zero\n        depth_img = np.zeros_like(disparity_img, dtype=np.float32)\n        valid_mask = disparity_img > 0\n        depth_img[valid_mask] = (baseline * focal_length) / disparity_img[valid_mask]\n\n        # Publish depth image\n        depth_msg = self.bridge.cv2_to_imgmsg(depth_img, "32FC1")\n        depth_msg.header = msg.header\n        self.depth_pub.publish(depth_msg)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-sensor-fusion",children:"Isaac ROS Sensor Fusion"}),"\n",(0,a.jsx)(n.p,{children:"Combining multiple sensor modalities for robust perception:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, LaserScan\nfrom geometry_msgs.msg import PoseWithCovarianceStamped\nfrom tf2_ros import TransformListener, Buffer\nimport numpy as np\n\nclass IsaacSensorFusion(Node):\n    def __init__(self):\n        super().__init__(\'isaac_sensor_fusion\')\n\n        # Initialize TF listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Subscribe to multiple sensors\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_rect_color\', self.image_callback, 10\n        )\n        self.lidar_sub = self.create_subscription(\n            PointCloud2, \'/velodyne_points\', self.lidar_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n\n        # Data storage\n        self.latest_image = None\n        self.latest_lidar = None\n        self.latest_scan = None\n\n        # Fusion timer\n        self.fusion_timer = self.create_timer(0.1, self.fusion_callback)\n\n    def image_callback(self, msg):\n        """Store latest image"""\n        self.latest_image = msg\n\n    def lidar_callback(self, msg):\n        """Process LiDAR data"""\n        self.latest_lidar = msg\n        # Convert PointCloud2 to usable format for fusion\n\n    def scan_callback(self, msg):\n        """Process laser scan data"""\n        self.latest_scan = msg\n        # Process laser scan for immediate obstacle detection\n\n    def fusion_callback(self):\n        """Fuse sensor data for comprehensive perception"""\n        if self.latest_image and self.latest_lidar:\n            # Project image data to 3D using depth\n            # Combine with LiDAR data\n            # Create fused perception output\n            fused_perception = self.fuse_image_lidar(\n                self.latest_image,\n                self.latest_lidar\n            )\n\n            # Publish fused results\n            self.publish_fused_data(fused_perception)\n\n    def fuse_image_lidar(self, image_msg, lidar_msg):\n        """Fuse image and LiDAR data"""\n        # Convert image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(image_msg, "bgr8")\n\n        # Extract 3D points from LiDAR\n        points_3d = self.extract_lidar_points(lidar_msg)\n\n        # Project 3D points to image plane\n        projected_points = self.project_3d_to_2d(points_3d, image_msg.header.frame_id)\n\n        # Combine semantic information from image with geometric data from LiDAR\n        fused_data = {\n            \'semantic_map\': self.get_semantic_info(cv_image),\n            \'geometric_map\': self.get_geometric_info(points_3d),\n            \'combined_map\': self.combine_maps(cv_image, points_3d)\n        }\n\n        return fused_data\n'})}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"tensorrt-optimization",children:"TensorRT Optimization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom isaac_ros_tensor_rt_interfaces.msg import EngineInfo\nimport tensorrt as trt\n\nclass OptimizedInferenceNode(Node):\n    def __init__(self):\n        super().__init__(\'optimized_inference\')\n\n        # Create TensorRT engine\n        self.trt_engine = self.load_trt_engine()\n\n        # Set up optimized pipeline\n        self.setup_optimized_pipeline()\n\n    def load_trt_engine(self):\n        """Load optimized TensorRT engine"""\n        # Load pre-built TensorRT engine\n        with open(\'model.plan\', \'rb\') as f:\n            engine_data = f.read()\n\n        runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))\n        engine = runtime.deserialize_cuda_engine(engine_data)\n\n        return engine\n\n    def setup_optimized_pipeline(self):\n        """Setup GPU-optimized processing pipeline"""\n        # Configure GPU memory pools\n        # Set up CUDA streams for parallel processing\n        # Optimize data transfers between CPU and GPU\n        pass\n'})}),"\n",(0,a.jsx)(n.h2,{id:"exercise-build-a-multi-modal-perception-system",children:"Exercise: Build a Multi-Modal Perception System"}),"\n",(0,a.jsx)(n.p,{children:"Create a perception system that:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Subscribes to camera, LiDAR, and IMU data"}),"\n",(0,a.jsx)(n.li,{children:"Performs object detection using Isaac ROS"}),"\n",(0,a.jsx)(n.li,{children:"Fuses sensor data for robust perception"}),"\n",(0,a.jsx)(n.li,{children:"Publishes a comprehensive environment model"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS provides powerful perception capabilities that enable robots to understand their environment through AI-powered processing. The GPU acceleration and optimized neural networks make real-time perception feasible for complex robotic applications. Proper sensor fusion techniques combine multiple modalities for robust and reliable perception."}),"\n",(0,a.jsx)(n.hr,{})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},8453(e,n,s){s.d(n,{R:()=>o,x:()=>r});var i=s(6540);const a={},t=i.createContext(a);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);