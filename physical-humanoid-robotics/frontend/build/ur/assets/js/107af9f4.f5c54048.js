"use strict";(globalThis.webpackChunkphysical_humanoid_robotics_book=globalThis.webpackChunkphysical_humanoid_robotics_book||[]).push([[593],{1455(e,n,a){a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/module-3/module-3-chapter-5","title":"Isaac AI Brain: Learning and Adaptation","description":"This chapter explores how NVIDIA Isaac enables machine learning and adaptation capabilities that allow robots to learn from experience and adapt to new situations.","source":"@site/content/modules/module-3/chapter-5.md","sourceDirName":"modules/module-3","slug":"/modules/module-3/module-3-chapter-5","permalink":"/ur/docs/modules/module-3/module-3-chapter-5","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-humanoid-robotics/physical-humanoid-robotics-book/tree/main/packages/create-docusaurus/templates/shared/content/modules/module-3/chapter-5.md","tags":[],"version":"current","frontMatter":{"id":"module-3-chapter-5","title":"Isaac AI Brain: Learning and Adaptation","sidebar_label":"Isaac Learning"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Manipulation","permalink":"/ur/docs/modules/module-3/module-3-chapter-4"},"next":{"title":"VLA Basics","permalink":"/ur/docs/modules/module-4/module-4-chapter-1"}}');var r=a(4848),s=a(8453);const i={id:"module-3-chapter-5",title:"Isaac AI Brain: Learning and Adaptation",sidebar_label:"Isaac Learning"},o="Isaac AI Brain: Learning and Adaptation",l={},c=[{value:"Isaac Learning Framework",id:"isaac-learning-framework",level:2},{value:"Isaac Reinforcement Learning",id:"isaac-reinforcement-learning",level:2},{value:"Deep Q-Network for Robot Control",id:"deep-q-network-for-robot-control",level:3},{value:"Isaac Imitation Learning",id:"isaac-imitation-learning",level:2},{value:"Learning from Demonstrations",id:"learning-from-demonstrations",level:3},{value:"Isaac Transfer Learning",id:"isaac-transfer-learning",level:2},{value:"Adapting Pre-trained Models",id:"adapting-pre-trained-models",level:3},{value:"Isaac Online Adaptation",id:"isaac-online-adaptation",level:2},{value:"Real-time Learning and Adjustment",id:"real-time-learning-and-adjustment",level:3},{value:"Exercise: Implement Adaptive Control System",id:"exercise-implement-adaptive-control-system",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"isaac-ai-brain-learning-and-adaptation",children:"Isaac AI Brain: Learning and Adaptation"})}),"\n",(0,r.jsx)(n.p,{children:"This chapter explores how NVIDIA Isaac enables machine learning and adaptation capabilities that allow robots to learn from experience and adapt to new situations."}),"\n",(0,r.jsx)(n.h2,{id:"isaac-learning-framework",children:"Isaac Learning Framework"}),"\n",(0,r.jsx)(n.p,{children:"Isaac Learning includes:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reinforcement Learning"}),": Training agents through interaction with environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Imitation Learning"}),": Learning from human demonstrations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transfer Learning"}),": Adapting pre-trained models to new tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Online Adaptation"}),": Real-time learning and adjustment"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"isaac-reinforcement-learning",children:"Isaac Reinforcement Learning"}),"\n",(0,r.jsx)(n.h3,{id:"deep-q-network-for-robot-control",children:"Deep Q-Network for Robot Control"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Float32\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\n\nclass IsaacDQN(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(IsaacDQN, self).__init__()\n        self.fc1 = nn.Linear(state_size, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass IsaacRLAgent(Node):\n    def __init__(self):\n        super().__init__(\'isaac_rl_agent\')\n\n        # Subscriptions\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.laser_callback, 10\n        )\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_rect_color\', self.image_callback, 10\n        )\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.reward_pub = self.create_publisher(Float32, \'/rl_reward\', 10)\n\n        # RL parameters\n        self.state_size = 360  # Laser scan points\n        self.action_size = 9   # Discrete actions\n        self.memory = deque(maxlen=10000)\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.gamma = 0.95\n\n        # Neural networks\n        self.q_network = IsaacDQN(self.state_size, self.action_size)\n        self.target_network = IsaacDQN(self.state_size, self.action_size)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n\n        # Training parameters\n        self.batch_size = 32\n        self.update_target_freq = 100\n        self.step_count = 0\n\n        # Robot state\n        self.current_scan = None\n        self.current_image = None\n        self.previous_action = 0\n        self.previous_reward = 0.0\n\n    def laser_callback(self, msg):\n        """Process laser scan for state representation"""\n        self.current_scan = np.array(msg.ranges)\n        # Replace invalid ranges with maximum range\n        self.current_scan[np.isnan(self.current_scan)] = msg.range_max\n        self.current_scan[np.isinf(self.current_scan)] = msg.range_max\n\n        # Take action based on current state\n        if self.current_scan is not None:\n            action = self.act(self.current_scan)\n            self.execute_action(action)\n\n    def act(self, state):\n        """Choose action using epsilon-greedy policy"""\n        if np.random.random() <= self.epsilon:\n            return random.randrange(self.action_size)\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.cpu().data.numpy())\n\n    def remember(self, state, action, reward, next_state, done):\n        """Store experience in replay memory"""\n        self.memory.append((state, action, reward, next_state, done))\n\n    def replay(self):\n        """Train the model on a batch of experiences"""\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states = torch.FloatTensor([e[0] for e in batch])\n        actions = torch.LongTensor([e[1] for e in batch])\n        rewards = torch.FloatTensor([e[2] for e in batch])\n        next_states = torch.FloatTensor([e[3] for e in batch])\n        dones = torch.BoolTensor([e[4] for e in batch])\n\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # Decay epsilon\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def update_target_network(self):\n        """Update target network with current network weights"""\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def calculate_reward(self, action, scan_data):\n        """Calculate reward based on robot state and action"""\n        reward = 0.0\n\n        # Avoid obstacles\n        min_distance = np.min(scan_data)\n        if min_distance < 0.5:\n            reward -= 1.0  # Penalty for being too close to obstacles\n\n        # Move forward reward\n        if action == 4:  # Forward action\n            reward += 0.1\n\n        # Progress reward (simplified)\n        reward += 0.01\n\n        return reward\n\n    def execute_action(self, action):\n        """Execute action and calculate reward"""\n        cmd_vel = Twist()\n\n        # Map discrete action to velocity commands\n        action_map = {\n            0: (-0.2, -0.5),  # Sharp left\n            1: (-0.1, -0.2),  # Left\n            2: (0.0, -0.1),   # Slight left\n            3: (-0.2, 0.0),   # Back left\n            4: (0.5, 0.0),    # Forward\n            5: (0.0, 0.1),    # Slight right\n            6: (0.2, 0.0),    # Back right\n            7: (0.1, 0.2),    # Right\n            8: (0.2, 0.5)     # Sharp right\n        }\n\n        if action in action_map:\n            cmd_vel.linear.x, cmd_vel.angular.z = action_map[action]\n\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Calculate and store reward\n        if self.current_scan is not None:\n            reward = self.calculate_reward(action, self.current_scan)\n            self.reward_pub.publish(Float32(data=reward))\n\n            # Store experience for training\n            if hasattr(self, \'previous_scan\'):\n                self.remember(self.previous_scan, self.previous_action,\n                            self.previous_reward, self.current_scan, False)\n\n            self.previous_action = action\n            self.previous_reward = reward\n            self.previous_scan = self.current_scan.copy()\n\n        # Train network periodically\n        if self.step_count % 10 == 0:\n            self.replay()\n\n        # Update target network periodically\n        if self.step_count % self.update_target_freq == 0:\n            self.update_target_network()\n\n        self.step_count += 1\n'})}),"\n",(0,r.jsx)(n.h2,{id:"isaac-imitation-learning",children:"Isaac Imitation Learning"}),"\n",(0,r.jsx)(n.h3,{id:"learning-from-demonstrations",children:"Learning from Demonstrations"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Bool\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nclass IsaacImitationNetwork(nn.Module):\n    def __init__(self, image_shape, laser_size, action_size):\n        super(IsaacImitationNetwork, self).__init__()\n\n        # CNN for image processing\n        self.conv1 = nn.Conv2d(image_shape[2], 32, kernel_size=8, stride=4)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n\n        # Calculate CNN output size\n        conv_out_size = self.calculate_conv_output_size(image_shape)\n\n        # FC layers combining image and laser features\n        self.fc1 = nn.Linear(conv_out_size + laser_size, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.action_head = nn.Linear(256, action_size)\n        self.value_head = nn.Linear(256, 1)\n\n    def calculate_conv_output_size(self, image_shape):\n        """Calculate the output size of convolutional layers"""\n        h, w, c = image_shape\n        h = (h - 8) // 4 + 1\n        h = (h - 4) // 2 + 1\n        h = (h - 3) // 1 + 1\n        w = (w - 8) // 4 + 1\n        w = (w - 4) // 2 + 1\n        w = (w - 3) // 1 + 1\n        return h * w * 64\n\n    def forward(self, image, laser):\n        # Process image through CNN\n        x = torch.relu(self.conv1(image))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        x = x.view(x.size(0), -1)  # Flatten\n\n        # Concatenate with laser data\n        combined = torch.cat([x, laser], dim=1)\n\n        # Process through FC layers\n        x = torch.relu(self.fc1(combined))\n        x = torch.relu(self.fc2(x))\n\n        # Output action and value\n        action = self.action_head(x)\n        value = self.value_head(x)\n\n        return action, value\n\nclass IsaacImitationLearner(Node):\n    def __init__(self):\n        super().__init__(\'isaac_imitation_learner\')\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_rect_color\', self.image_callback, 10\n        )\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.laser_callback, 10\n        )\n        self.expert_cmd_sub = self.create_subscription(\n            Twist, \'/expert_cmd_vel\', self.expert_command_callback, 10\n        )\n        self.demo_start_sub = self.create_subscription(\n            Bool, \'/start_demonstration\', self.start_demonstration_callback, 10\n        )\n\n        # Publishers\n        self.agent_cmd_pub = self.create_publisher(Twist, \'/agent_cmd_vel\', 10)\n\n        # Network and training\n        self.network = IsaacImitationNetwork(\n            image_shape=(480, 640, 3),  # H, W, C\n            laser_size=360,\n            action_size=2  # linear.x, angular.z\n        )\n        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=0.001)\n\n        # Data collection\n        self.demonstration_data = []\n        self.is_collecting = False\n        self.current_image = None\n        self.current_laser = None\n        self.current_expert_cmd = None\n\n    def image_callback(self, msg):\n        """Process image data"""\n        # Convert ROS Image to PyTorch tensor\n        image_np = self.ros_image_to_numpy(msg)\n        self.current_image = torch.FloatTensor(image_np).permute(2, 0, 1).unsqueeze(0)  # CHW format\n\n    def laser_callback(self, msg):\n        """Process laser scan data"""\n        laser_data = np.array(msg.ranges)\n        laser_data[np.isnan(laser_data)] = msg.range_max\n        laser_data[np.isinf(laser_data)] = msg.range_max\n        self.current_laser = torch.FloatTensor(laser_data).unsqueeze(0)\n\n    def expert_command_callback(self, msg):\n        """Store expert demonstration commands"""\n        self.current_expert_cmd = torch.FloatTensor([msg.linear.x, msg.angular.z]).unsqueeze(0)\n\n    def start_demonstration_callback(self, msg):\n        """Start/stop demonstration collection"""\n        self.is_collecting = msg.data\n        if not self.is_collecting:\n            # Train on collected data\n            self.train_on_demonstrations()\n\n    def collect_demonstration_step(self):\n        """Collect one step of demonstration"""\n        if (self.is_collecting and\n            self.current_image is not None and\n            self.current_laser is not None and\n            self.current_expert_cmd is not None):\n\n            self.demonstration_data.append({\n                \'image\': self.current_image.clone(),\n                \'laser\': self.current_laser.clone(),\n                \'command\': self.current_expert_cmd.clone()\n            })\n\n    def train_on_demonstrations(self):\n        """Train network on collected demonstrations"""\n        if len(self.demonstration_data) == 0:\n            return\n\n        for epoch in range(10):  # Train for 10 epochs\n            total_loss = 0\n            for data in self.demonstration_data:\n                # Forward pass\n                predicted_action, _ = self.network(data[\'image\'], data[\'laser\'])\n\n                # Calculate loss (MSE between predicted and expert action)\n                loss = nn.MSELoss()(predicted_action, data[\'command\'])\n\n                # Backward pass\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                total_loss += loss.item()\n\n            self.get_logger().info(f\'Epoch {epoch}, Average Loss: {total_loss/len(self.demonstration_data)}\')\n\n        # Clear demonstration data after training\n        self.demonstration_data = []\n\n    def execute_policy(self):\n        """Execute learned policy"""\n        if (self.current_image is not None and\n            self.current_laser is not None):\n\n            with torch.no_grad():\n                action, _ = self.network(self.current_image, self.current_laser)\n\n            cmd_vel = Twist()\n            cmd_vel.linear.x = action[0, 0].item()\n            cmd_vel.angular.z = action[0, 1].item()\n\n            self.agent_cmd_pub.publish(cmd_vel)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"isaac-transfer-learning",children:"Isaac Transfer Learning"}),"\n",(0,r.jsx)(n.h3,{id:"adapting-pre-trained-models",children:"Adapting Pre-trained Models"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\n\nclass IsaacTransferLearner(Node):\n    def __init__(self):\n        super().__init__(\'isaac_transfer_learner\')\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_rect_color\', self.image_callback, 10\n        )\n\n        # Publishers\n        self.prediction_pub = self.create_publisher(String, \'/transfer_prediction\', 10)\n\n        # Load pre-trained model\n        self.pretrained_model = models.resnet18(pretrained=True)\n\n        # Replace final layer for specific task (e.g., object classification in robotics)\n        num_features = self.pretrained_model.fc.in_features\n        self.pretrained_model.fc = nn.Linear(num_features, 10)  # 10 classes for robotics objects\n\n        # Freeze early layers (transfer learning approach)\n        for param in self.pretrained_model.parameters():\n            param.requires_grad = False\n\n        # Only train the final layer\n        for param in self.pretrained_model.fc.parameters():\n            param.requires_grad = True\n\n        self.optimizer = torch.optim.Adam(self.pretrained_model.fc.parameters(), lr=0.001)\n        self.criterion = nn.CrossEntropyLoss()\n\n        self.current_image = None\n        self.is_training = False\n\n    def image_callback(self, msg):\n        """Process image and make predictions"""\n        image_tensor = self.ros_image_to_tensor(msg)\n\n        with torch.no_grad():\n            output = self.pretrained_model(image_tensor)\n            predicted_class = torch.argmax(output, dim=1)\n\n        # Publish prediction\n        pred_msg = String()\n        pred_msg.data = f"Class {predicted_class.item()}"\n        self.prediction_pub.publish(pred_msg)\n\n    def fine_tune_model(self, new_dataset):\n        """Fine-tune the model on new robotic-specific data"""\n        self.pretrained_model.train()\n\n        for epoch in range(5):  # Few epochs for fine-tuning\n            total_loss = 0\n            for batch_idx, (data, target) in enumerate(new_dataset):\n                self.optimizer.zero_grad()\n                output = self.pretrained_model(data)\n                loss = self.criterion(output, target)\n                loss.backward()\n                self.optimizer.step()\n                total_loss += loss.item()\n\n            self.get_logger().info(f\'Fine-tuning Epoch {epoch}, Loss: {total_loss/len(new_dataset)}\')\n\n        # Now unfreeze more layers for further training if needed\n        for param in list(self.pretrained_model.parameters())[-5:]:  # Unfreeze last 5 layers\n            param.requires_grad = True\n'})}),"\n",(0,r.jsx)(n.h2,{id:"isaac-online-adaptation",children:"Isaac Online Adaptation"}),"\n",(0,r.jsx)(n.h3,{id:"real-time-learning-and-adjustment",children:"Real-time Learning and Adjustment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Imu\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Float32MultiArray\nimport numpy as np\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nclass IsaacOnlineAdapter(Node):\n    def __init__(self):\n        super().__init__(\'isaac_online_adapter\')\n\n        # Subscriptions\n        self.joint_state_sub = self.create_subscription(\n            JointState, \'/joint_states\', self.joint_state_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            Twist, \'/cmd_vel\', self.command_callback, 10\n        )\n\n        # Publishers\n        self.adapted_cmd_pub = self.create_publisher(Twist, \'/adapted_cmd_vel\', 10)\n        self.adaptation_params_pub = self.create_publisher(\n            Float32MultiArray, \'/adaptation_params\', 10\n        )\n\n        # Online learning components\n        self.feature_scaler = StandardScaler()\n        self.adaptation_model = SGDRegressor(\n            learning_rate=\'adaptive\',\n            eta0=0.01,\n            max_iter=1\n        )\n\n        # Robot state tracking\n        self.joint_positions = np.zeros(18)\n        self.joint_velocities = np.zeros(18)\n        self.imu_data = np.zeros(6)  # [angular_vel, linear_acc]\n        self.command_history = []\n        self.performance_history = []\n\n        # Adaptation parameters\n        self.adaptation_enabled = True\n        self.performance_threshold = 0.8\n        self.adaptation_rate = 0.1\n\n    def joint_state_callback(self, msg):\n        """Update joint state information"""\n        for i, name in enumerate(msg.name):\n            # Update joint positions and velocities\n            if i < len(self.joint_positions):\n                self.joint_positions[i] = msg.position[i]\n            if i < len(self.joint_velocities):\n                self.joint_velocities[i] = msg.velocity[i]\n\n    def imu_callback(self, msg):\n        """Update IMU data"""\n        self.imu_data[:3] = [\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        ]\n        self.imu_data[3:] = [\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z\n        ]\n\n    def command_callback(self, msg):\n        """Process command and adapt if needed"""\n        # Create feature vector from current state\n        features = self.create_feature_vector(msg)\n\n        # Apply adaptation if enabled\n        if self.adaptation_enabled:\n            adapted_cmd = self.apply_adaptation(msg, features)\n        else:\n            adapted_cmd = msg\n\n        # Publish adapted command\n        self.adapted_cmd_pub.publish(adapted_cmd)\n\n        # Update adaptation model with new data\n        self.update_adaptation_model(features, msg, adapted_cmd)\n\n    def create_feature_vector(self, command):\n        """Create feature vector for adaptation"""\n        features = np.concatenate([\n            self.joint_positions[:6],      # First 6 joints for simplicity\n            self.joint_velocities[:6],     # Velocities\n            self.imu_data,                 # IMU data\n            [command.linear.x, command.linear.y, command.linear.z],\n            [command.angular.x, command.angular.y, command.angular.z]\n        ])\n\n        # Normalize features\n        if len(self.command_history) > 10:  # Enough data for scaling\n            features = self.feature_scaler.transform(features.reshape(1, -1)).flatten()\n        else:\n            self.feature_scaler.partial_fit(features.reshape(1, -1))\n\n        return features\n\n    def apply_adaptation(self, original_cmd, features):\n        """Apply online adaptation to command"""\n        # Predict adaptation offsets using the model\n        if hasattr(self.adaptation_model, \'coef_\'):\n            adaptation_offset = self.adaptation_model.predict(features.reshape(1, -1))[0]\n\n            # Apply adaptation to command\n            adapted_cmd = Twist()\n            adapted_cmd.linear.x = original_cmd.linear.x + adaptation_offset * 0.1\n            adapted_cmd.angular.z = original_cmd.angular.z + adaptation_offset * 0.05\n\n            # Constrain to safe limits\n            adapted_cmd.linear.x = np.clip(adapted_cmd.linear.x, -1.0, 1.0)\n            adapted_cmd.angular.z = np.clip(adapted_cmd.angular.z, -1.0, 1.0)\n\n            return adapted_cmd\n        else:\n            return original_cmd\n\n    def update_adaptation_model(self, features, original_cmd, adapted_cmd):\n        """Update adaptation model with new experience"""\n        # Calculate performance based on command execution\n        performance = self.calculate_performance(original_cmd, adapted_cmd)\n\n        # Store for history\n        self.command_history.append((features, original_cmd))\n        self.performance_history.append(performance)\n\n        # Keep only recent history\n        if len(self.command_history) > 1000:\n            self.command_history.pop(0)\n            self.performance_history.pop(0)\n\n        # Update model if we have enough good examples\n        if len(self.performance_history) > 10 and np.mean(self.performance_history[-10:]) > self.performance_threshold:\n            # Use the most recent data to update the model\n            recent_features = np.array([x[0] for x in self.command_history[-10:]])\n            recent_performance = np.array(self.performance_history[-10:])\n\n            # Partial fit for online learning\n            self.adaptation_model.partial_fit(recent_features, recent_performance)\n\n    def calculate_performance(self, original_cmd, adapted_cmd):\n        """Calculate performance metric for adaptation"""\n        # This would typically involve comparing expected vs actual results\n        # For simplicity, using a placeholder based on IMU stability\n        stability_score = 1.0 - min(0.5, np.linalg.norm(self.imu_data[3:])/10.0)  # Linear acceleration\n        return stability_score\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-implement-adaptive-control-system",children:"Exercise: Implement Adaptive Control System"}),"\n",(0,r.jsx)(n.p,{children:"Create a system that:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Learns from human demonstrations"}),"\n",(0,r.jsx)(n.li,{children:"Adapts its behavior based on environmental feedback"}),"\n",(0,r.jsx)(n.li,{children:"Uses transfer learning to apply knowledge to new tasks"}),"\n",(0,r.jsx)(n.li,{children:"Continuously improves through online learning"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Isaac provides powerful learning and adaptation capabilities that enable robots to improve their performance over time. Through reinforcement learning, imitation learning, transfer learning, and online adaptation, robots can become more capable and efficient as they gain experience. These AI brain capabilities are essential for creating truly autonomous and adaptable robotic systems."}),"\n",(0,r.jsx)(n.hr,{})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,n,a){a.d(n,{R:()=>i,x:()=>o});var t=a(6540);const r={},s=t.createContext(r);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);