"use strict";(globalThis.webpackChunkphysical_humanoid_robotics_book=globalThis.webpackChunkphysical_humanoid_robotics_book||[]).push([[170],{7660(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"modules/module-5/module-5-chapter-1","title":"Autonomous Humanoid Project Overview","description":"Welcome to Module 5: Capstone - Autonomous Humanoid Project. This capstone module integrates all previous modules to create a comprehensive autonomous humanoid robot system capable of complex tasks through multimodal perception, AI-driven decision making, and precise control.","source":"@site/content/modules/module-5/chapter-1.md","sourceDirName":"modules/module-5","slug":"/modules/module-5/module-5-chapter-1","permalink":"/docs/modules/module-5/module-5-chapter-1","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-humanoid-robotics/physical-humanoid-robotics-book/tree/main/packages/create-docusaurus/templates/shared/content/modules/module-5/chapter-1.md","tags":[],"version":"current","frontMatter":{"id":"module-5-chapter-1","title":"Autonomous Humanoid Project Overview","sidebar_label":"Project Overview"},"sidebar":"tutorialSidebar","previous":{"title":"VLA Deployment","permalink":"/docs/modules/module-4/module-4-chapter-5"},"next":{"title":"Control Systems","permalink":"/docs/modules/module-5/module-5-chapter-2"}}');var r=t(4848),a=t(8453);const s={id:"module-5-chapter-1",title:"Autonomous Humanoid Project Overview",sidebar_label:"Project Overview"},i="Autonomous Humanoid Project Overview",l={},c=[{value:"Project Architecture",id:"project-architecture",level:2},{value:"Humanoid Robot Specifications",id:"humanoid-robot-specifications",level:2},{value:"Physical Characteristics",id:"physical-characteristics",level:3},{value:"ROS 2 Architecture for Humanoid Robot",id:"ros-2-architecture-for-humanoid-robot",level:2},{value:"Humanoid Robot ROS 2 Nodes",id:"humanoid-robot-ros-2-nodes",level:3},{value:"Balance Control System",id:"balance-control-system",level:2},{value:"Whole-Body Balance Controller",id:"whole-body-balance-controller",level:3},{value:"Walking Pattern Generation",id:"walking-pattern-generation",level:2},{value:"Bipedal Walking Controller",id:"bipedal-walking-controller",level:3},{value:"Exercise: Design Humanoid Robot Architecture",id:"exercise-design-humanoid-robot-architecture",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"autonomous-humanoid-project-overview",children:"Autonomous Humanoid Project Overview"})}),"\n",(0,r.jsx)(n.p,{children:"Welcome to Module 5: Capstone - Autonomous Humanoid Project. This capstone module integrates all previous modules to create a comprehensive autonomous humanoid robot system capable of complex tasks through multimodal perception, AI-driven decision making, and precise control."}),"\n",(0,r.jsx)(n.h2,{id:"project-architecture",children:"Project Architecture"}),"\n",(0,r.jsx)(n.p,{children:"The autonomous humanoid robot system integrates:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Module 1"}),": Robotic nervous system (ROS 2)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Module 2"}),": Digital twin (Gazebo & Unity)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Module 3"}),": AI-brain (NVIDIA Isaac)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Module 4"}),": Vision-Language-Action (VLA) systems"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Autonomous Humanoid Robot                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Perception Layer                                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Vision        \u2502   Audio         \u2502   Tactile & Proprio     \u2502 \u2502\n\u2502  \u2502   (Cameras,     \u2502   (Microphones, \u2502   (IMU, Joint Encoders, \u2502 \u2502\n\u2502  \u2502   Depth, LIDAR) \u2502   Speakers)     \u2502   Force Sensors)       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                 \u2502\n\u2502  AI Brain Layer                                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  NVIDIA Isaac ROS  \u2502  VLA Systems   \u2502  Deep Learning      \u2502 \u2502\n\u2502  \u2502  Perception &      \u2502  Vision-Language\u2502  Models             \u2502 \u2502\n\u2502  \u2502  Planning         \u2502  Action         \u2502                     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                 \u2502\n\u2502  Control Layer                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Locomotion    \u2502   Manipulation  \u2502   Whole-Body Control   \u2502 \u2502\n\u2502  \u2502   (Walking,     \u2502   (Grasping,    \u2502   (Balance, Posture,   \u2502 \u2502\n\u2502  \u2502   Balance)      \u2502   Manipulation) \u2502   Coordination)       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                                 \u2502\n\u2502  Communication Layer                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502              ROS 2 Communication Framework                  \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502 Navigation\u2502 \u2502 Planning  \u2502 \u2502 Control   \u2502 \u2502 Perception\u2502  \u2502 \u2502\n\u2502  \u2502  \u2502           \u2502 \u2502           \u2502 \u2502           \u2502 \u2502           \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"humanoid-robot-specifications",children:"Humanoid Robot Specifications"}),"\n",(0,r.jsx)(n.h3,{id:"physical-characteristics",children:"Physical Characteristics"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class HumanoidSpecifications:\n    def __init__(self):\n        # Physical dimensions\n        self.height = 1.5  # meters\n        self.weight = 30   # kg\n        self.dof = 24      # degrees of freedom\n\n        # Joint configuration\n        self.joints = {\n            # Legs (6 DOF each leg)\n            'left_hip_yaw': {'range': (-1.5, 1.5), 'speed': 2.0},\n            'left_hip_roll': {'range': (-0.5, 0.5), 'speed': 2.0},\n            'left_hip_pitch': {'range': (-2.0, 0.5), 'speed': 2.0},\n            'left_knee': {'range': (0.0, 2.5), 'speed': 2.0},\n            'left_ankle_pitch': {'range': (-0.5, 0.5), 'speed': 1.5},\n            'left_ankle_roll': {'range': (-0.3, 0.3), 'speed': 1.5},\n\n            'right_hip_yaw': {'range': (-1.5, 1.5), 'speed': 2.0},\n            'right_hip_roll': {'range': (-0.5, 0.5), 'speed': 2.0},\n            'right_hip_pitch': {'range': (-2.0, 0.5), 'speed': 2.0},\n            'right_knee': {'range': (0.0, 2.5), 'speed': 2.0},\n            'right_ankle_pitch': {'range': (-0.5, 0.5), 'speed': 1.5},\n            'right_ankle_roll': {'range': (-0.3, 0.3), 'speed': 1.5},\n\n            # Arms (5 DOF each arm)\n            'left_shoulder_pitch': {'range': (-2.0, 2.0), 'speed': 3.0},\n            'left_shoulder_roll': {'range': (0.0, 2.5), 'speed': 3.0},\n            'left_elbow': {'range': (0.0, 2.5), 'speed': 3.0},\n            'left_wrist_yaw': {'range': (-1.5, 1.5), 'speed': 4.0},\n            'left_wrist_pitch': {'range': (-1.0, 1.0), 'speed': 4.0},\n\n            'right_shoulder_pitch': {'range': (-2.0, 2.0), 'speed': 3.0},\n            'right_shoulder_roll': {'range': (-2.5, 0.0), 'speed': 3.0},\n            'right_elbow': {'range': (0.0, 2.5), 'speed': 3.0},\n            'right_wrist_yaw': {'range': (-1.5, 1.5), 'speed': 4.0},\n            'right_wrist_pitch': {'range': (-1.0, 1.0), 'speed': 4.0},\n\n            # Head (2 DOF)\n            'neck_yaw': {'range': (-1.5, 1.5), 'speed': 5.0},\n            'neck_pitch': {'range': (-0.5, 0.5), 'speed': 5.0}\n        }\n\n        # Sensors\n        self.sensors = {\n            'cameras': {\n                'stereo_camera': {'resolution': (640, 480), 'fov': 60},\n                'head_camera': {'resolution': (1280, 720), 'fov': 90}\n            },\n            'lidar': {'range': 10.0, 'resolution': 0.5},\n            'imu': {'rate': 100, 'accuracy': 'high'},\n            'force_torque': {'range': 100.0, 'rate': 1000}\n        }\n\n        # Computing resources\n        self.computing = {\n            'cpu': 'ARM A78 8-core',\n            'gpu': 'NVIDIA Jetson Orin AGX',\n            'memory': '32GB LPDDR5',\n            'storage': '512GB NVMe SSD'\n        }\n"})}),"\n",(0,r.jsx)(n.h2,{id:"ros-2-architecture-for-humanoid-robot",children:"ROS 2 Architecture for Humanoid Robot"}),"\n",(0,r.jsx)(n.h3,{id:"humanoid-robot-ros-2-nodes",children:"Humanoid Robot ROS 2 Nodes"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Image, Imu, PointCloud2\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom std_msgs.msg import String, Bool\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom control_msgs.msg import JointTrajectoryControllerState\nimport numpy as np\n\nclass HumanoidRobotNode(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_robot\')\n\n        # Initialize humanoid specifications\n        self.specs = HumanoidSpecifications()\n\n        # Joint state management\n        self.joint_states = JointState()\n        self.joint_states.name = list(self.specs.joints.keys())\n        self.joint_states.position = [0.0] * len(self.joint_states.name)\n        self.joint_states.velocity = [0.0] * len(self.joint_states.name)\n        self.joint_states.effort = [0.0] * len(self.joint_states.name)\n\n        # ROS 2 interfaces\n        self.joint_state_pub = self.create_publisher(JointState, \'/joint_states\', 10)\n        self.joint_cmd_pub = self.create_publisher(JointTrajectory, \'/joint_trajectory_controller/joint_trajectory\', 10)\n\n        # Sensor subscriptions\n        self.imu_sub = self.create_subscription(Imu, \'/imu/data\', self.imu_callback, 10)\n        self.camera_sub = self.create_subscription(Image, \'/camera/image_raw\', self.camera_callback, 10)\n        self.lidar_sub = self.create_subscription(PointCloud2, \'/lidar/points\', self.lidar_callback, 10)\n\n        # Command interfaces\n        self.cmd_vel_sub = self.create_subscription(Twist, \'/cmd_vel\', self.cmd_vel_callback, 10)\n        self.behavior_sub = self.create_subscription(String, \'/behavior_command\', self.behavior_callback, 10)\n\n        # State management\n        self.current_behavior = "idle"\n        self.balance_controller = BalanceController(self.specs)\n        self.walk_controller = WalkController(self.specs)\n        self.manipulation_controller = ManipulationController(self.specs)\n\n        # Timer for state publishing\n        self.state_timer = self.create_timer(0.01, self.publish_state)  # 100Hz\n\n    def imu_callback(self, msg):\n        """Process IMU data for balance control"""\n        self.balance_controller.update_imu_data(msg)\n\n    def camera_callback(self, msg):\n        """Process camera data for perception"""\n        # Forward to perception system\n        self.perception_system.process_image(msg)\n\n    def lidar_callback(self, msg):\n        """Process LIDAR data for navigation"""\n        # Forward to navigation system\n        self.navigation_system.process_lidar(msg)\n\n    def cmd_vel_callback(self, msg):\n        """Process velocity commands"""\n        if self.current_behavior == "walking":\n            self.walk_controller.set_target_velocity(msg.linear.x, msg.angular.z)\n        elif self.current_behavior == "manipulation":\n            self.manipulation_controller.set_target_velocity(msg)\n\n    def behavior_callback(self, msg):\n        """Process behavior commands"""\n        self.set_behavior(msg.data)\n\n    def set_behavior(self, behavior):\n        """Switch between different behaviors"""\n        if behavior in ["idle", "walking", "manipulation", "balance"]:\n            self.current_behavior = behavior\n            self.get_logger().info(f\'Switched to behavior: {behavior}\')\n\n    def publish_state(self):\n        """Publish current robot state"""\n        # Update joint states from controllers\n        self.joint_states.header.stamp = self.get_clock().now().to_msg()\n\n        # Get current positions from controllers\n        current_positions = self.balance_controller.get_current_positions()\n        if current_positions:\n            self.joint_states.position = current_positions\n\n        # Publish joint states\n        self.joint_state_pub.publish(self.joint_states)\n\n        # Publish behavior status\n        behavior_status = String()\n        behavior_status.data = self.current_behavior\n        self.behavior_status_pub.publish(behavior_status)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"balance-control-system",children:"Balance Control System"}),"\n",(0,r.jsx)(n.h3,{id:"whole-body-balance-controller",children:"Whole-Body Balance Controller"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy import signal\nimport math\n\nclass BalanceController:\n    def __init__(self, specs):\n        self.specs = specs\n        self.imu_data = {'orientation': [0, 0, 0, 1], 'angular_velocity': [0, 0, 0], 'linear_acceleration': [0, 0, 9.81]}\n        self.target_com = np.array([0, 0, 0.8])  # Center of mass target\n        self.current_com = np.array([0, 0, 0.8])\n        self.com_velocity = np.array([0, 0, 0])\n        self.com_acceleration = np.array([0, 0, 0])\n\n        # PID controllers for balance\n        self.com_pid = {\n            'x': {'kp': 100.0, 'ki': 10.0, 'kd': 10.0, 'integral': 0, 'prev_error': 0},\n            'y': {'kp': 100.0, 'ki': 10.0, 'kd': 10.0, 'integral': 0, 'prev_error': 0},\n            'z': {'kp': 50.0, 'ki': 5.0, 'kd': 5.0, 'integral': 0, 'prev_error': 0}\n        }\n\n        # Zero moment point (ZMP) controller\n        self.zmp_controller = ZMPController(specs)\n\n    def update_imu_data(self, imu_msg):\n        \"\"\"Update IMU data for balance control\"\"\"\n        self.imu_data['orientation'] = [\n            imu_msg.orientation.x,\n            imu_msg.orientation.y,\n            imu_msg.orientation.z,\n            imu_msg.orientation.w\n        ]\n        self.imu_data['angular_velocity'] = [\n            imu_msg.angular_velocity.x,\n            imu_msg.angular_velocity.y,\n            imu_msg.angular_velocity.z\n        ]\n        self.imu_data['linear_acceleration'] = [\n            imu_msg.linear_acceleration.x,\n            imu_msg.linear_acceleration.y,\n            imu_msg.linear_acceleration.z\n        ]\n\n    def compute_balance_corrections(self):\n        \"\"\"Compute balance corrections based on current state\"\"\"\n        # Calculate current center of mass position\n        current_orientation = self.imu_data['orientation']\n        current_ang_vel = self.imu_data['angular_velocity']\n        current_lin_acc = self.imu_data['linear_acceleration']\n\n        # Convert quaternion to Euler angles for balance calculation\n        roll, pitch, yaw = self.quaternion_to_euler(current_orientation)\n\n        # Calculate balance errors\n        com_error = self.target_com - self.current_com\n        orientation_error = np.array([roll, pitch, yaw])\n\n        # Apply PID control for balance\n        balance_corrections = {}\n        for axis, idx in [('x', 0), ('y', 1), ('z', 2)]:\n            error = com_error[idx]\n            dt = 0.01  # Assume 100Hz control loop\n\n            # PID calculations\n            self.com_pid[axis]['integral'] += error * dt\n            derivative = (error - self.com_pid[axis]['prev_error']) / dt\n\n            correction = (self.com_pid[axis]['kp'] * error +\n                         self.com_pid[axis]['ki'] * self.com_pid[axis]['integral'] +\n                         self.com_pid[axis]['kd'] * derivative)\n\n            self.com_pid[axis]['prev_error'] = error\n            balance_corrections[axis] = correction\n\n        # Apply ZMP-based corrections\n        zmp_corrections = self.zmp_controller.compute_corrections(\n            self.current_com, self.com_velocity, self.com_acceleration\n        )\n\n        return balance_corrections, zmp_corrections\n\n    def quaternion_to_euler(self, q):\n        \"\"\"Convert quaternion to Euler angles (roll, pitch, yaw)\"\"\"\n        w, x, y, z = q\n\n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = math.atan2(sinr_cosp, cosr_cosp)\n\n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = math.copysign(math.pi / 2, sinp)  # Use 90 degrees if out of range\n        else:\n            pitch = math.asin(sinp)\n\n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = math.atan2(siny_cosp, cosy_cosp)\n\n        return roll, pitch, yaw\n\n    def get_current_positions(self):\n        \"\"\"Get current joint positions with balance corrections\"\"\"\n        # This would integrate with the actual joint controllers\n        # For now, return current positions\n        return self.current_joint_positions\n"})}),"\n",(0,r.jsx)(n.h2,{id:"walking-pattern-generation",children:"Walking Pattern Generation"}),"\n",(0,r.jsx)(n.h3,{id:"bipedal-walking-controller",children:"Bipedal Walking Controller"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy import signal\nimport math\n\nclass WalkController:\n    def __init__(self, specs):\n        self.specs = specs\n        self.target_velocity = [0.0, 0.0]  # [linear_x, angular_z]\n        self.current_phase = 0.0\n        self.step_frequency = 1.0  # steps per second\n        self.step_length = 0.3  # meters\n        self.step_height = 0.05  # meters\n        self.support_leg = 'left'  # Which leg supports weight\n\n        # Walking pattern parameters\n        self.gait_params = {\n            'stance_duration': 0.6,  # 60% stance phase\n            'swing_duration': 0.4,   # 40% swing phase\n            'double_support_ratio': 0.1  # 10% double support\n        }\n\n    def set_target_velocity(self, linear_x, angular_z):\n        \"\"\"Set target walking velocity\"\"\"\n        self.target_velocity[0] = linear_x\n        self.target_velocity[1] = angular_z\n\n        # Adjust gait parameters based on velocity\n        if abs(linear_x) > 0.1:\n            self.step_frequency = 0.5 + abs(linear_x) * 2.0  # Faster for higher speeds\n            self.step_length = min(0.4, abs(linear_x) * 0.6)  # Longer steps for higher speeds\n\n    def generate_step_trajectory(self, dt=0.01):\n        \"\"\"Generate trajectory for next step\"\"\"\n        # Calculate current gait phase\n        self.current_phase += dt * self.step_frequency\n        if self.current_phase > 1.0:\n            self.current_phase = 0.0\n            # Switch support leg\n            self.support_leg = 'right' if self.support_leg == 'left' else 'left'\n\n        # Generate swing leg trajectory\n        swing_trajectory = self.generate_swing_trajectory(self.current_phase)\n\n        # Generate stance leg trajectory (support leg)\n        stance_trajectory = self.generate_stance_trajectory(self.current_phase)\n\n        return {\n            'swing_leg': swing_trajectory,\n            'stance_leg': stance_trajectory,\n            'phase': self.current_phase,\n            'support_leg': self.support_leg\n        }\n\n    def generate_swing_trajectory(self, phase):\n        \"\"\"Generate trajectory for swing leg (foot that's swinging)\"\"\"\n        # Use cycloid trajectory for smooth stepping\n        # Phase 0.0 to 1.0 corresponds to one complete step cycle\n\n        # Calculate swing phase (when foot is off ground)\n        stance_end = self.gait_params['stance_duration']\n        double_support_end = stance_end + self.gait_params['double_support_ratio']\n\n        if phase < stance_end or phase > double_support_end:\n            # Stance phase - foot on ground\n            return self.calculate_stance_foot_position(phase)\n        else:\n            # Swing phase - foot moving\n            swing_phase = (phase - stance_end) / (1.0 - stance_end - self.gait_params['double_support_ratio'])\n\n            # Cycloid trajectory for foot\n            x_offset = self.step_length * swing_phase\n            y_offset = 0  # Side-to-side movement\n            z_height = self.step_height * (1 - math.cos(math.pi * swing_phase))  # Vertical lift\n\n            # Add lateral movement for turning\n            if self.target_velocity[1] != 0:\n                y_offset = self.target_velocity[1] * 0.1 * math.sin(math.pi * swing_phase)\n\n            return {\n                'x': x_offset,\n                'y': y_offset,\n                'z': z_height,\n                'trajectory_type': 'swing'\n            }\n\n    def generate_stance_trajectory(self, phase):\n        \"\"\"Generate trajectory for stance leg (foot that supports weight)\"\"\"\n        # Stance leg moves forward to prepare for next step\n        stance_end = self.gait_params['stance_duration']\n\n        if phase < stance_end:\n            # Stance phase - foot moves to prepare for next step\n            progress = phase / stance_end\n            x_offset = self.step_length * progress\n            return {\n                'x': x_offset,\n                'y': 0,\n                'z': 0,\n                'trajectory_type': 'stance'\n            }\n        else:\n            # Double support phase - both feet on ground temporarily\n            return {\n                'x': self.step_length,\n                'y': 0,\n                'z': 0,\n                'trajectory_type': 'double_support'\n            }\n\n    def calculate_joint_angles(self, foot_trajectory, leg_type):\n        \"\"\"Calculate joint angles for leg to reach foot trajectory\"\"\"\n        # This would implement inverse kinematics\n        # For now, return a simplified approximation\n        x, y, z = foot_trajectory['x'], foot_trajectory['y'], foot_trajectory['z']\n\n        # Simplified inverse kinematics for planar 3-DOF leg\n        # hip_x, hip_y, knee angles to reach (x, y, z) relative to hip\n        leg_length = 0.5  # Simplified leg length\n\n        # Calculate hip and knee angles to reach desired foot position\n        target_distance = math.sqrt(x**2 + (leg_length - z)**2)\n\n        if target_distance > 2 * leg_length:\n            # Target unreachable, return neutral position\n            return [0, 0, 0]  # hip_pitch, knee, ankle\n\n        # Knee angle\n        knee_angle = math.pi - math.acos(min(1.0, (2 * leg_length**2 - target_distance**2) / (2 * leg_length**2)))\n\n        # Hip angle\n        hip_angle = math.atan2(x, leg_length - z) - math.asin((leg_length * math.sin(knee_angle)) / target_distance)\n\n        # Ankle angle for balance\n        ankle_angle = -hip_angle - knee_angle  # Keep foot horizontal\n\n        return [hip_angle, knee_angle, ankle_angle]\n"})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-design-humanoid-robot-architecture",children:"Exercise: Design Humanoid Robot Architecture"}),"\n",(0,r.jsx)(n.p,{children:"Design a complete architecture for an autonomous humanoid robot that:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Integrates all four modules (ROS 2, Digital Twin, Isaac AI, VLA)"}),"\n",(0,r.jsx)(n.li,{children:"Defines the communication protocols between subsystems"}),"\n",(0,r.jsx)(n.li,{children:"Specifies the control hierarchy for different behaviors"}),"\n",(0,r.jsx)(n.li,{children:"Outlines the safety and validation mechanisms"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"The autonomous humanoid robot project represents the integration of all previous modules into a cohesive system. By combining robust ROS 2 communication, sophisticated AI perception and planning, and precise control systems, we can create humanoid robots capable of complex autonomous behaviors. The architecture must balance computational requirements, real-time performance, and safety considerations while enabling rich human-robot interaction."}),"\n",(0,r.jsx)(n.hr,{})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>i});var o=t(6540);const r={},a=o.createContext(r);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);