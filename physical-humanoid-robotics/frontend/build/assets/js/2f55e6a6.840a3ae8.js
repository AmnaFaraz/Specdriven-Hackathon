"use strict";(globalThis.webpackChunkphysical_humanoid_robotics_book=globalThis.webpackChunkphysical_humanoid_robotics_book||[]).push([[329],{6201(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"modules/module-4/module-4-chapter-4","title":"Training VLA Models for Robotic Applications","description":"This chapter explores the methodologies and techniques for training Vision-Language-Action (VLA) models specifically for robotic applications, covering data collection, model architectures, and training strategies.","source":"@site/content/modules/module-4/chapter-4.md","sourceDirName":"modules/module-4","slug":"/modules/module-4/module-4-chapter-4","permalink":"/docs/modules/module-4/module-4-chapter-4","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-humanoid-robotics/physical-humanoid-robotics-book/tree/main/packages/create-docusaurus/templates/shared/content/modules/module-4/chapter-4.md","tags":[],"version":"current","frontMatter":{"id":"module-4-chapter-4","title":"Training VLA Models for Robotic Applications","sidebar_label":"VLA Training"},"sidebar":"tutorialSidebar","previous":{"title":"VLA Actions","permalink":"/docs/modules/module-4/module-4-chapter-3"},"next":{"title":"VLA Deployment","permalink":"/docs/modules/module-4/module-4-chapter-5"}}');var a=t(4848),i=t(8453);const s={id:"module-4-chapter-4",title:"Training VLA Models for Robotic Applications",sidebar_label:"VLA Training"},r="Training VLA Models for Robotic Applications",l={},d=[{value:"Data Collection for VLA Training",id:"data-collection-for-vla-training",level:2},{value:"Robot Interaction Dataset",id:"robot-interaction-dataset",level:3},{value:"Multi-modal Data Augmentation",id:"multi-modal-data-augmentation",level:3},{value:"VLA Model Training Framework",id:"vla-model-training-framework",level:2},{value:"Contrastive Learning for VLA",id:"contrastive-learning-for-vla",level:3},{value:"Behavioral Cloning for Action Learning",id:"behavioral-cloning-for-action-learning",level:3},{value:"Imitation Learning for Robotics",id:"imitation-learning-for-robotics",level:2},{value:"Imitation Learning Framework",id:"imitation-learning-framework",level:3},{value:"Reinforcement Learning Integration",id:"reinforcement-learning-integration",level:2},{value:"VLA with Reinforcement Learning",id:"vla-with-reinforcement-learning",level:3},{value:"Training Loop and Evaluation",id:"training-loop-and-evaluation",level:2},{value:"Complete Training Pipeline",id:"complete-training-pipeline",level:3},{value:"Exercise: Implement VLA Training Pipeline",id:"exercise-implement-vla-training-pipeline",level:2},{value:"Summary",id:"summary",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"training-vla-models-for-robotic-applications",children:"Training VLA Models for Robotic Applications"})}),"\n",(0,a.jsx)(e.p,{children:"This chapter explores the methodologies and techniques for training Vision-Language-Action (VLA) models specifically for robotic applications, covering data collection, model architectures, and training strategies."}),"\n",(0,a.jsx)(e.h2,{id:"data-collection-for-vla-training",children:"Data Collection for VLA Training"}),"\n",(0,a.jsx)(e.h3,{id:"robot-interaction-dataset",children:"Robot Interaction Dataset"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\nimport json\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\n\nclass RobotInteractionDataset(Dataset):\n    def __init__(self, data_dir, transforms=None):\n        \"\"\"\n        Dataset for robot interaction data containing:\n        - Images (RGB, depth, semantic segmentation)\n        - Natural language commands\n        - Executed actions\n        - Success/failure labels\n        \"\"\"\n        self.data_dir = data_dir\n        self.transforms = transforms\n\n        # Load metadata\n        with open(f'{data_dir}/metadata.json', 'r') as f:\n            self.metadata = json.load(f)\n\n        self.episodes = self.metadata['episodes']\n        self.annotations = self.metadata['annotations']\n\n    def __len__(self):\n        return len(self.episodes)\n\n    def __getitem__(self, idx):\n        episode = self.episodes[idx]\n        episode_id = episode['episode_id']\n\n        # Load image\n        img_path = f\"{self.data_dir}/images/{episode_id}.jpg\"\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Load depth image\n        depth_path = f\"{self.data_dir}/depth/{episode_id}.png\"\n        depth = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)\n\n        # Load command\n        command = episode['command']\n\n        # Load action\n        action = np.array(episode['action'])  # [linear_x, angular_z, gripper_pos, etc.]\n\n        # Load success label\n        success = episode['success']\n\n        # Apply transforms\n        if self.transforms:\n            image = self.transforms(image)\n\n        # Process command to tokens\n        tokens = self.tokenize_command(command)\n\n        return {\n            'image': torch.FloatTensor(image),\n            'depth': torch.FloatTensor(depth),\n            'command': torch.LongTensor(tokens),\n            'action': torch.FloatTensor(action),\n            'success': torch.FloatTensor([success]),\n            'episode_id': episode_id\n        }\n\n    def tokenize_command(self, command):\n        \"\"\"Simple tokenization for robot commands\"\"\"\n        # This would typically use a proper tokenizer\n        vocab = {\n            'go': 1, 'to': 2, 'the': 3, 'red': 4, 'box': 5,\n            'pick': 6, 'up': 7, 'blue': 8, 'ball': 9, 'left': 10,\n            'right': 11, 'front': 12, 'behind': 13, 'near': 14,\n            'stop': 15, 'move': 16, 'turn': 17, 'approach': 18,\n            'avoid': 19, 'grasp': 20, 'release': 21, 'lift': 22\n        }\n\n        tokens = []\n        for word in command.lower().split():\n            tokens.append(vocab.get(word, 0))  # 0 for unknown words\n\n        # Pad to fixed length\n        max_length = 20\n        if len(tokens) < max_length:\n            tokens.extend([0] * (max_length - len(tokens)))\n        else:\n            tokens = tokens[:max_length]\n\n        return tokens\n\n# Data loading example\ndef create_robot_dataloader(data_dir, batch_size=32, shuffle=True):\n    \"\"\"Create dataloader for robot interaction data\"\"\"\n    dataset = RobotInteractionDataset(data_dir)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"})}),"\n",(0,a.jsx)(e.h3,{id:"multi-modal-data-augmentation",children:"Multi-modal Data Augmentation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torchvision.transforms as transforms\nimport numpy as np\nimport cv2\n\nclass MultiModalAugmentation:\n    def __init__(self, image_size=224):\n        self.image_size = image_size\n\n        # Image augmentation\n        self.image_transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomRotation(degrees=10),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        # Depth augmentation\n        self.depth_transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor()\n        ])\n\n    def augment_image(self, image):\n        """Apply image augmentation"""\n        # Convert to PIL for torchvision transforms\n        from PIL import Image\n        pil_image = Image.fromarray(image)\n        return self.image_transform(pil_image)\n\n    def augment_depth(self, depth):\n        """Apply depth augmentation"""\n        return self.depth_transform(depth)\n\n    def augment_command(self, command):\n        """Apply text augmentation"""\n        # Synonym replacement, paraphrasing, etc.\n        # For now, return original command\n        return command\n\n    def __call__(self, image, depth, command):\n        """Apply augmentations to multi-modal data"""\n        aug_image = self.augment_image(image)\n        aug_depth = self.augment_depth(depth)\n        aug_command = self.augment_command(command)\n\n        return aug_image, aug_depth, aug_command\n'})}),"\n",(0,a.jsx)(e.h2,{id:"vla-model-training-framework",children:"VLA Model Training Framework"}),"\n",(0,a.jsx)(e.h3,{id:"contrastive-learning-for-vla",children:"Contrastive Learning for VLA"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\n\nclass VLATrainingFramework(nn.Module):\n    def __init__(self, vision_model, language_model, action_model,\n                 temperature=0.07, action_weight=1.0):\n        super().__init__()\n\n        # Core models\n        self.vision_model = vision_model\n        self.language_model = language_model\n        self.action_model = action_model\n\n        # Projection layers\n        self.vision_projection = nn.Linear(512, 512)\n        self.text_projection = nn.Linear(512, 512)\n        self.action_projection = nn.Linear(20, 512)  # 20-dim action space\n\n        # Temperature for contrastive loss\n        self.temperature = temperature\n        self.action_weight = action_weight\n\n        # Scaler for mixed precision training\n        self.scaler = GradScaler()\n\n    def forward(self, images, texts, actions, attention_mask=None):\n        """Forward pass for VLA training"""\n        # Encode vision\n        vision_features = self.vision_model(images)\n        vision_features = self.vision_projection(vision_features)\n        vision_features = F.normalize(vision_features, dim=-1)\n\n        # Encode text\n        text_features = self.language_model(texts, attention_mask=attention_mask)\n        text_features = self.text_projection(text_features)\n        text_features = F.normalize(text_features, dim=-1)\n\n        # Encode actions\n        action_features = self.action_model(actions)\n        action_features = self.action_projection(action_features)\n        action_features = F.normalize(action_features, dim=-1)\n\n        # Compute contrastive losses\n        vision_text_loss = self.contrastive_loss(vision_features, text_features)\n        vision_action_loss = self.contrastive_loss(vision_features, action_features)\n        text_action_loss = self.contrastive_loss(text_features, action_features)\n\n        # Action prediction loss\n        action_pred_loss = F.mse_loss(\n            self.action_model(vision_features, text_features),\n            actions\n        )\n\n        # Combined loss\n        total_loss = (vision_text_loss +\n                     vision_action_loss +\n                     text_action_loss +\n                     self.action_weight * action_pred_loss)\n\n        return {\n            \'total_loss\': total_loss,\n            \'vision_text_loss\': vision_text_loss,\n            \'vision_action_loss\': vision_action_loss,\n            \'text_action_loss\': text_action_loss,\n            \'action_loss\': action_pred_loss\n        }\n\n    def contrastive_loss(self, feat1, feat2):\n        """Compute contrastive loss between two feature sets"""\n        # Similarity matrix\n        sim_matrix = torch.matmul(feat1, feat2.t()) / self.temperature\n\n        # Labels (diagonal elements are positive pairs)\n        labels = torch.arange(sim_matrix.size(0)).to(sim_matrix.device)\n\n        # Cross entropy loss\n        loss_i = F.cross_entropy(sim_matrix, labels)\n        loss_t = F.cross_entropy(sim_matrix.t(), labels)\n\n        return (loss_i + loss_t) / 2\n\n    def train_step(self, images, texts, actions, optimizer, scheduler=None):\n        """Single training step with mixed precision"""\n        optimizer.zero_grad()\n\n        with autocast():\n            losses = self.forward(images, texts, actions)\n            total_loss = losses[\'total_loss\']\n\n        # Scale loss and backward\n        self.scaler.scale(total_loss).backward()\n\n        # Unscaled optimizer step\n        self.scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n\n        self.scaler.step(optimizer)\n        self.scaler.update()\n\n        if scheduler:\n            scheduler.step()\n\n        return losses\n'})}),"\n",(0,a.jsx)(e.h3,{id:"behavioral-cloning-for-action-learning",children:"Behavioral Cloning for Action Learning"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BehavioralCloning(nn.Module):\n    def __init__(self, vision_dim=512, text_dim=512, action_dim=7, hidden_dim=256):\n        super().__init__()\n\n        # Vision and text encoders (could be pre-trained)\n        self.vision_encoder = nn.Sequential(\n            nn.Linear(vision_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n\n        self.text_encoder = nn.Sequential(\n            nn.Linear(text_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n\n        # Fusion layer\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU()\n        )\n\n        # Action decoder\n        self.action_decoder = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, action_dim),\n            nn.Tanh()  # Normalize to [-1, 1]\n        )\n\n    def forward(self, vision_features, text_features):\n        """Predict action from vision and text features"""\n        # Encode vision and text\n        encoded_vision = self.vision_encoder(vision_features)\n        encoded_text = self.text_encoder(text_features)\n\n        # Normalize features\n        encoded_vision = F.normalize(encoded_vision, dim=-1)\n        encoded_text = F.normalize(encoded_text, dim=-1)\n\n        # Fuse features\n        fused_features = torch.cat([encoded_vision, encoded_text], dim=-1)\n        fused_features = self.fusion(fused_features)\n\n        # Decode action\n        predicted_action = self.action_decoder(fused_features)\n\n        return predicted_action\n\n    def compute_loss(self, predicted_actions, target_actions):\n        """Compute behavioral cloning loss"""\n        # MSE loss for continuous actions\n        mse_loss = F.mse_loss(predicted_actions, target_actions)\n\n        # Optional: Add regularization\n        l2_reg = sum(p.pow(2).sum() for p in self.parameters())\n\n        total_loss = mse_loss + 0.001 * l2_reg\n        return total_loss\n'})}),"\n",(0,a.jsx)(e.h2,{id:"imitation-learning-for-robotics",children:"Imitation Learning for Robotics"}),"\n",(0,a.jsx)(e.h3,{id:"imitation-learning-framework",children:"Imitation Learning Framework"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\nclass ImitationLearningFramework:\n    def __init__(self, model, learning_rate=1e-4, device='cuda'):\n        self.model = model\n        self.device = device\n        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer, mode='min', factor=0.5, patience=10\n        )\n\n        self.model.to(device)\n\n    def train_epoch(self, dataloader, epoch_num):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        total_loss = 0\n        num_batches = 0\n\n        for batch_idx, batch in enumerate(dataloader):\n            # Move data to device\n            images = batch['image'].to(self.device)\n            commands = batch['command'].to(self.device)\n            actions = batch['action'].to(self.device)\n\n            # Forward pass\n            predicted_actions = self.model(images, commands)\n\n            # Compute loss\n            loss = F.mse_loss(predicted_actions, actions)\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n\n            total_loss += loss.item()\n            num_batches += 1\n\n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch_num}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n\n        avg_loss = total_loss / num_batches\n        self.scheduler.step(avg_loss)\n\n        return avg_loss\n\n    def evaluate(self, dataloader):\n        \"\"\"Evaluate the model\"\"\"\n        self.model.eval()\n        total_loss = 0\n        num_samples = 0\n\n        with torch.no_grad():\n            for batch in dataloader:\n                images = batch['image'].to(self.device)\n                commands = batch['command'].to(self.device)\n                actions = batch['action'].to(self.device)\n\n                predicted_actions = self.model(images, commands)\n                loss = F.mse_loss(predicted_actions, actions)\n\n                total_loss += loss.item() * len(actions)\n                num_samples += len(actions)\n\n        avg_loss = total_loss / num_samples\n        return avg_loss\n\n    def save_checkpoint(self, filepath, epoch, train_loss, val_loss):\n        \"\"\"Save model checkpoint\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'train_loss': train_loss,\n            'val_loss': val_loss\n        }\n        torch.save(checkpoint, filepath)\n\n    def load_checkpoint(self, filepath):\n        \"\"\"Load model checkpoint\"\"\"\n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        return checkpoint['epoch'], checkpoint['train_loss'], checkpoint['val_loss']\n"})}),"\n",(0,a.jsx)(e.h2,{id:"reinforcement-learning-integration",children:"Reinforcement Learning Integration"}),"\n",(0,a.jsx)(e.h3,{id:"vla-with-reinforcement-learning",children:"VLA with Reinforcement Learning"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass VLAReinforcementLearning(nn.Module):\n    def __init__(self, vision_dim=512, text_dim=512, action_dim=7, hidden_dim=256):\n        super().__init__()\n\n        # Actor network\n        self.actor = nn.Sequential(\n            nn.Linear(vision_dim + text_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()  # Actions in [-1, 1]\n        )\n\n        # Critic network\n        self.critic = nn.Sequential(\n            nn.Linear(vision_dim + text_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)  # Value function\n        )\n\n        # Action space parameters\n        self.action_dim = action_dim\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n\n    def forward(self, vision_features, text_features):\n        """Get action and value"""\n        # Combine features\n        combined_features = torch.cat([vision_features, text_features], dim=-1)\n\n        # Actor: predict action mean\n        action_mean = self.actor(combined_features)\n\n        # Reparameterization trick for sampling\n        std = torch.exp(self.log_std)\n        noise = torch.randn_like(action_mean) * std\n        action = action_mean + noise\n\n        # Clip actions\n        action = torch.clamp(action, -1, 1)\n\n        # Critic: predict value\n        action_cat = torch.cat([combined_features, action], dim=-1)\n        value = self.critic(action_cat)\n\n        return action, action_mean, std, value\n\n    def get_action_logprob(self, vision_features, text_features):\n        """Get action and log probability for policy gradient"""\n        action, action_mean, std, _ = self.forward(vision_features, text_features)\n\n        # Calculate log probability\n        var = std.pow(2)\n        log_prob = -((action - action_mean).pow(2) / (2 * var) + torch.log(std * np.sqrt(2 * np.pi))).sum(-1)\n\n        return action, log_prob\n\n    def evaluate_actions(self, vision_features, text_features, actions):\n        """Evaluate actions for PPO"""\n        _, action_mean, std, value = self.forward(vision_features, text_features)\n\n        # Calculate log probability\n        var = std.pow(2)\n        log_prob = -((actions - action_mean).pow(2) / (2 * var) + torch.log(std * np.sqrt(2 * np.pi))).sum(-1)\n\n        return log_prob, value\n'})}),"\n",(0,a.jsx)(e.h2,{id:"training-loop-and-evaluation",children:"Training Loop and Evaluation"}),"\n",(0,a.jsx)(e.h3,{id:"complete-training-pipeline",children:"Complete Training Pipeline"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nfrom torch.utils.tensorboard import SummaryWriter\nimport os\nfrom tqdm import tqdm\n\nclass VLATrainingPipeline:\n    def __init__(self, model, train_loader, val_loader, device='cuda'):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n\n        # Optimizers\n        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100)\n\n        # Logging\n        self.writer = SummaryWriter(log_dir='./logs/vla_training')\n        self.best_val_loss = float('inf')\n\n    def train(self, num_epochs=100, save_dir='./checkpoints'):\n        \"\"\"Main training loop\"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n\n        for epoch in range(num_epochs):\n            # Training phase\n            train_loss = self.train_epoch(epoch)\n\n            # Validation phase\n            val_loss = self.validate_epoch(epoch)\n\n            # Learning rate scheduling\n            self.scheduler.step(val_loss)\n\n            # Logging\n            self.writer.add_scalar('Loss/Train', train_loss, epoch)\n            self.writer.add_scalar('Loss/Val', val_loss, epoch)\n            self.writer.add_scalar('Learning_Rate', self.scheduler.get_last_lr()[0], epoch)\n\n            print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n\n            # Save best model\n            if val_loss < self.best_val_loss:\n                self.best_val_loss = val_loss\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_loss': val_loss\n                }, f'{save_dir}/best_model.pth')\n\n            # Save checkpoint every 10 epochs\n            if epoch % 10 == 0:\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_loss': val_loss\n                }, f'{save_dir}/checkpoint_epoch_{epoch}.pth')\n\n    def train_epoch(self, epoch):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        total_loss = 0\n        num_batches = 0\n\n        progress_bar = tqdm(self.train_loader, desc=f'Training Epoch {epoch}')\n        for batch in progress_bar:\n            # Move data to device\n            images = batch['image'].to(self.device)\n            commands = batch['command'].to(self.device)\n            actions = batch['action'].to(self.device)\n\n            # Forward pass\n            outputs = self.model(images, commands, actions)\n\n            # Compute loss\n            loss = outputs['total_loss']\n\n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            self.optimizer.step()\n\n            total_loss += loss.item()\n            num_batches += 1\n\n            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n\n        return total_loss / num_batches\n\n    def validate_epoch(self, epoch):\n        \"\"\"Validate for one epoch\"\"\"\n        self.model.eval()\n        total_loss = 0\n        num_batches = 0\n\n        with torch.no_grad():\n            for batch in self.val_loader:\n                images = batch['image'].to(self.device)\n                commands = batch['command'].to(self.device)\n                actions = batch['action'].to(self.device)\n\n                outputs = self.model(images, commands, actions)\n                loss = outputs['total_loss']\n\n                total_loss += loss.item()\n                num_batches += 1\n\n        return total_loss / num_batches\n"})}),"\n",(0,a.jsx)(e.h2,{id:"exercise-implement-vla-training-pipeline",children:"Exercise: Implement VLA Training Pipeline"}),"\n",(0,a.jsx)(e.p,{children:"Create a complete training pipeline that:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Loads robot interaction data with images, commands, and actions"}),"\n",(0,a.jsx)(e.li,{children:"Implements contrastive learning for vision-language alignment"}),"\n",(0,a.jsx)(e.li,{children:"Trains action prediction with behavioral cloning"}),"\n",(0,a.jsx)(e.li,{children:"Evaluates model performance on held-out data"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Training VLA models for robotic applications requires careful consideration of data collection, model architecture, and training objectives. The combination of contrastive learning for vision-language alignment, behavioral cloning for action learning, and potentially reinforcement learning creates robust models capable of understanding complex multimodal inputs and generating appropriate actions. Proper evaluation and validation ensure that trained models generalize well to novel situations."}),"\n",(0,a.jsx)(e.hr,{})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453(n,e,t){t.d(e,{R:()=>s,x:()=>r});var o=t(6540);const a={},i=o.createContext(a);function s(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);