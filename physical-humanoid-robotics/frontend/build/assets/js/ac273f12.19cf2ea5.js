"use strict";(globalThis.webpackChunkphysical_humanoid_robotics_book=globalThis.webpackChunkphysical_humanoid_robotics_book||[]).push([[540],{2157(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"modules/module-4/module-4-chapter-2","title":"Implementing Vision-Language Models for Robotics","description":"This chapter focuses on practical implementation of vision-language models specifically tailored for robotic applications, with emphasis on real-time performance and embodied intelligence.","source":"@site/content/modules/module-4/chapter-2.md","sourceDirName":"modules/module-4","slug":"/modules/module-4/module-4-chapter-2","permalink":"/docs/modules/module-4/module-4-chapter-2","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-humanoid-robotics/physical-humanoid-robotics-book/tree/main/packages/create-docusaurus/templates/shared/content/modules/module-4/chapter-2.md","tags":[],"version":"current","frontMatter":{"id":"module-4-chapter-2","title":"Implementing Vision-Language Models for Robotics","sidebar_label":"VLA Implementation"},"sidebar":"tutorialSidebar","previous":{"title":"VLA Basics","permalink":"/docs/modules/module-4/module-4-chapter-1"},"next":{"title":"VLA Actions","permalink":"/docs/modules/module-4/module-4-chapter-3"}}');var o=t(4848),s=t(8453);const a={id:"module-4-chapter-2",title:"Implementing Vision-Language Models for Robotics",sidebar_label:"VLA Implementation"},r="Implementing Vision-Language Models for Robotics",l={},d=[{value:"Vision-Language Model Architectures",id:"vision-language-model-architectures",level:2},{value:"CLIP-Based Architecture for Robotics",id:"clip-based-architecture-for-robotics",level:3},{value:"Vision Transformer for Robotics",id:"vision-transformer-for-robotics",level:3},{value:"Real-time VLA Processing Pipeline",id:"real-time-vla-processing-pipeline",level:2},{value:"Efficient VLA Pipeline for Robotics",id:"efficient-vla-pipeline-for-robotics",level:3},{value:"Semantic Scene Understanding",id:"semantic-scene-understanding",level:2},{value:"Scene Understanding with VLA",id:"scene-understanding-with-vla",level:3},{value:"Language Grounding in Visual Context",id:"language-grounding-in-visual-context",level:2},{value:"Grounded Language Understanding",id:"grounded-language-understanding",level:3},{value:"Exercise: Implement Scene-Aware Command Following",id:"exercise-implement-scene-aware-command-following",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"implementing-vision-language-models-for-robotics",children:"Implementing Vision-Language Models for Robotics"})}),"\n",(0,o.jsx)(n.p,{children:"This chapter focuses on practical implementation of vision-language models specifically tailored for robotic applications, with emphasis on real-time performance and embodied intelligence."}),"\n",(0,o.jsx)(n.h2,{id:"vision-language-model-architectures",children:"Vision-Language Model Architectures"}),"\n",(0,o.jsx)(n.h3,{id:"clip-based-architecture-for-robotics",children:"CLIP-Based Architecture for Robotics"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom transformers import CLIPVisionModel, CLIPTextModel, CLIPProcessor\nimport numpy as np\n\nclass RobotCLIP(nn.Module):\n    def __init__(self, vision_model_name="openai/clip-vit-base-patch32",\n                 text_model_name="openai/clip-vit-base-patch32"):\n        super().__init__()\n\n        # Vision encoder\n        self.vision_model = CLIPVisionModel.from_pretrained(vision_model_name)\n\n        # Text encoder\n        self.text_model = CLIPTextModel.from_pretrained(text_model_name)\n\n        # Vision and text projection layers\n        self.vision_projection = nn.Linear(512, 512)\n        self.text_projection = nn.Linear(512, 512)\n\n        # Temperature parameter for contrastive loss\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        # Action prediction head\n        self.action_head = nn.Sequential(\n            nn.Linear(1024, 512),  # Combined vision-text features\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 20)  # 20 possible robot actions\n        )\n\n    def forward(self, pixel_values, input_ids, attention_mask):\n        """Forward pass for vision-language understanding"""\n        # Encode images\n        vision_outputs = self.vision_model(pixel_values=pixel_values)\n        vision_features = vision_outputs.pooler_output\n        vision_features = self.vision_projection(vision_features)\n\n        # Normalize vision features\n        vision_features = vision_features / vision_features.norm(dim=-1, keepdim=True)\n\n        # Encode text\n        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.pooler_output\n        text_features = self.text_projection(text_features)\n\n        # Normalize text features\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n        # Compute similarity\n        logit_scale = self.logit_scale.exp()\n        logits_per_text = torch.matmul(text_features, vision_features.t()) * logit_scale\n        logits_per_image = logits_per_text.t()\n\n        # Combine features for action prediction\n        combined_features = torch.cat([vision_features, text_features], dim=-1)\n        action_logits = self.action_head(combined_features)\n\n        return {\n            \'logits_per_image\': logits_per_image,\n            \'logits_per_text\': logits_per_text,\n            \'action_logits\': action_logits,\n            \'vision_features\': vision_features,\n            \'text_features\': text_features\n        }\n\n    def encode_image(self, pixel_values):\n        """Encode image to feature vector"""\n        with torch.no_grad():\n            vision_outputs = self.vision_model(pixel_values=pixel_values)\n            vision_features = vision_outputs.pooler_output\n            vision_features = self.vision_projection(vision_features)\n            vision_features = vision_features / vision_features.norm(dim=-1, keepdim=True)\n        return vision_features\n\n    def encode_text(self, input_ids, attention_mask):\n        """Encode text to feature vector"""\n        with torch.no_grad():\n            text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n            text_features = text_outputs.pooler_output\n            text_features = self.text_projection(text_features)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        return text_features\n'})}),"\n",(0,o.jsx)(n.h3,{id:"vision-transformer-for-robotics",children:"Vision Transformer for Robotics"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom einops import rearrange\nimport torch.nn.functional as F\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, image_size=224, patch_size=16, num_classes=1000, dim=768, depth=12, heads=12, mlp_dim=3072, dropout=0.1, emb_dropout=0.1):\n        super().__init__()\n\n        assert image_size % patch_size == 0, \'Image dimensions must be divisible by the patch size.\'\n\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = 3 * patch_size ** 2\n\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        # Patch embedding\n        self.to_patch_embedding = nn.Sequential(\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            nn.LayerNorm(dim),\n        )\n\n        # Positional embedding\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        # Transformer layers\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=dim,\n                nhead=heads,\n                dim_feedforward=mlp_dim,\n                dropout=dropout,\n                batch_first=True\n            ),\n            num_layers=depth\n        )\n\n        # Classification head\n        self.to_latent = nn.Identity()\n        self.mlp_head = nn.Linear(dim, num_classes)\n\n    def forward(self, img):\n        """Forward pass through Vision Transformer"""\n        p = self.patch_size\n\n        # Convert image to patches\n        x = rearrange(img, \'b c (h p1) (w p2) -> b (h w) (p1 p2 c)\', p1=p, p2=p)\n        x = self.to_patch_embedding(x)\n\n        # Add class token and positional embedding\n        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(x.shape[1] + 1)]\n        x = self.dropout(x)\n\n        # Transformer encoder\n        x = self.transformer(x)\n\n        # Take the class token for classification\n        x = x[:, 0]\n        x = self.to_latent(x)\n        return self.mlp_head(x)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-vla-processing-pipeline",children:"Real-time VLA Processing Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"efficient-vla-pipeline-for-robotics",children:"Efficient VLA Pipeline for Robotics"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CompressedImage\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport torch\nimport time\nfrom queue import Queue\nimport threading\n\nclass EfficientVLAPipeline(Node):\n    def __init__(self):\n        super().__init__(\'efficient_vla_pipeline\')\n\n        # Initialize models\n        self.vla_model = RobotCLIP()\n        self.vla_model.eval()  # Set to evaluation mode\n\n        # ROS 2 interfaces\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_rect_color\', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, \'/robot_command\', self.command_callback, 10\n        )\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Processing components\n        self.bridge = CvBridge()\n        self.image_queue = Queue(maxsize=2)  # Limit queue size for real-time performance\n        self.command_queue = Queue(maxsize=5)\n\n        # State management\n        self.current_command = ""\n        self.command_lock = threading.Lock()\n        self.last_process_time = time.time()\n\n        # Processing thread\n        self.processing_thread = threading.Thread(target=self.processing_loop)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n    def image_callback(self, msg):\n        """Process incoming image with rate limiting"""\n        current_time = time.time()\n\n        # Limit processing rate to 10Hz for efficiency\n        if current_time - self.last_process_time > 0.1:\n            try:\n                cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n                # Add to processing queue if not full\n                if not self.image_queue.full():\n                    self.image_queue.put(cv_image)\n                    self.last_process_time = current_time\n            except Exception as e:\n                self.get_logger().error(f\'Error processing image: {e}\')\n\n    def command_callback(self, msg):\n        """Process incoming command"""\n        if not self.command_queue.full():\n            self.command_queue.put(msg.data)\n\n    def processing_loop(self):\n        """Background processing loop"""\n        while rclpy.ok():\n            try:\n                # Process image if available\n                if not self.image_queue.empty() and not self.command_queue.empty():\n                    image = self.image_queue.get()\n                    command = self.command_queue.get()\n\n                    # Process with VLA model\n                    action = self.process_vla_request(image, command)\n\n                    # Execute action\n                    self.execute_action(action)\n\n            except Exception as e:\n                self.get_logger().error(f\'Error in processing loop: {e}\')\n\n            time.sleep(0.01)  # Small delay to prevent busy waiting\n\n    def process_vla_request(self, image, command):\n        """Process vision-language request using VLA model"""\n        try:\n            # Preprocess image (resize and normalize)\n            import cv2\n            image_resized = cv2.resize(image, (224, 224))\n            image_tensor = torch.from_numpy(image_resized).float().permute(2, 0, 1).unsqueeze(0) / 255.0\n\n            # Preprocess command\n            from transformers import CLIPTokenizer\n            tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")\n            text_inputs = tokenizer(\n                command,\n                return_tensors="pt",\n                padding=True,\n                truncation=True,\n                max_length=77\n            )\n\n            # Run VLA model\n            with torch.no_grad():\n                outputs = self.vla_model(\n                    pixel_values=image_tensor,\n                    input_ids=text_inputs[\'input_ids\'],\n                    attention_mask=text_inputs[\'attention_mask\']\n                )\n\n                # Get action prediction\n                action_logits = outputs[\'action_logits\']\n                action_idx = torch.argmax(action_logits, dim=-1).item()\n\n            return self.action_index_to_command(action_idx)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing VLA request: {e}\')\n            return ("stop", 0.0, 0.0)\n\n    def action_index_to_command(self, action_idx):\n        """Map action index to robot command"""\n        # Define action mapping for robot tasks\n        action_map = {\n            0: ("move_forward", 0.3, 0.0),\n            1: ("move_backward", -0.3, 0.0),\n            2: ("turn_left", 0.0, 0.3),\n            3: ("turn_right", 0.0, -0.3),\n            4: ("approach_object", 0.1, 0.0),\n            5: ("avoid_obstacle", -0.1, 0.2),\n            6: ("stop", 0.0, 0.0),\n            # Add more actions as needed\n        }\n\n        return action_map.get(action_idx, ("stop", 0.0, 0.0))\n\n    def execute_action(self, action):\n        """Execute the determined action"""\n        cmd_vel = Twist()\n        action_type, linear_vel, angular_vel = action\n\n        cmd_vel.linear.x = linear_vel\n        cmd_vel.angular.z = angular_vel\n\n        self.cmd_vel_pub.publish(cmd_vel)\n        self.get_logger().info(f\'Executing: {action_type} (linear: {linear_vel}, angular: {angular_vel})\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"semantic-scene-understanding",children:"Semantic Scene Understanding"}),"\n",(0,o.jsx)(n.h3,{id:"scene-understanding-with-vla",children:"Scene Understanding with VLA"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\nimport numpy as np\nimport cv2\n\nclass VLASceneUnderstanding(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # CLIPSeg for semantic segmentation\n        self.processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")\n        self.model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined")\n\n        # Object detection model (YOLO or similar)\n        # This would typically be integrated with a detection model\n\n        # Scene classification head\n        self.scene_classifier = nn.Linear(512, 10)  # 10 scene types\n\n    def forward(self, pixel_values, texts):\n        """Forward pass for scene understanding"""\n        # Process with CLIPSeg\n        outputs = self.model(pixel_values=pixel_values, prompt=texts)\n        masks = outputs.logits  # [batch_size, num_prompts, height, width]\n\n        return masks\n\n    def segment_objects(self, image, object_prompts):\n        """Segment specific objects in the image"""\n        inputs = self.processor(\n            text=object_prompts,\n            images=[image] * len(object_prompts),\n            padding=True,\n            return_tensors="pt"\n        )\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            masks = outputs.logits\n\n        # Convert masks to binary segmentation\n        binary_masks = torch.sigmoid(masks) > 0.5\n\n        return binary_masks, masks\n\n    def get_scene_description(self, image, scene_prompts):\n        """Get semantic description of the scene"""\n        inputs = self.processor(\n            text=scene_prompts,\n            images=[image] * len(scene_prompts),\n            padding=True,\n            return_tensors="pt"\n        )\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            scores = torch.sigmoid(outputs.logits)\n\n        # Get the most likely scene description\n        scene_idx = torch.argmax(scores.mean(dim=[2, 3]))  # Average over spatial dimensions\n        return scene_prompts[scene_idx], scores[scene_idx]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"language-grounding-in-visual-context",children:"Language Grounding in Visual Context"}),"\n",(0,o.jsx)(n.h3,{id:"grounded-language-understanding",children:"Grounded Language Understanding"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass GroundedLanguageUnderstanding(nn.Module):\n    def __init__(self, vocab_size=30522, hidden_dim=512, spatial_dim=256):\n        super().__init__()\n\n        # Language encoder\n        self.language_encoder = nn.Embedding(vocab_size, hidden_dim)\n        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n\n        # Spatial attention for grounding\n        self.spatial_attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim,\n            num_heads=8\n        )\n\n        # Visual-linguistic fusion\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_dim, spatial_dim)  # Map to spatial coordinates\n        )\n\n        # Output heads\n        self.position_head = nn.Linear(spatial_dim, 2)  # x, y coordinates\n        self.object_head = nn.Linear(spatial_dim, 50)  # object class prediction\n\n    def forward(self, text_tokens, visual_features):\n        """Ground language in visual context"""\n        # Encode text\n        text_embeddings = self.language_encoder(text_tokens)\n        text_features, _ = self.lstm(text_embeddings)\n\n        # Attend visual features based on text\n        attended_visual, attention_weights = self.spatial_attention(\n            text_features.transpose(0, 1),  # Query from text\n            visual_features.transpose(0, 1),  # Key from visual\n            visual_features.transpose(0, 1)   # Value from visual\n        )\n\n        # Fuse text and attended visual features\n        fused_features = torch.cat([\n            text_features[:, -1, :],  # Last text token\n            attended_visual[-1, :, :]  # Last attended visual\n        ], dim=-1)\n\n        # Apply fusion layer\n        fusion_output = self.fusion_layer(fused_features)\n\n        # Predict spatial position and object\n        position = self.position_head(fusion_output)\n        object_class = self.object_head(fusion_output)\n\n        return {\n            \'position\': position,\n            \'object_class\': object_class,\n            \'attention_weights\': attention_weights\n        }\n\n    def process_language_command(self, command, visual_features):\n        """Process a language command in visual context"""\n        # Tokenize command (simplified)\n        tokens = self.tokenize_command(command)\n        tokens_tensor = torch.tensor([tokens]).long()\n\n        # Forward pass\n        result = self.forward(tokens_tensor, visual_features)\n\n        return result\n\n    def tokenize_command(self, command):\n        """Simple tokenization for robot commands"""\n        # This would use a proper tokenizer in practice\n        # For now, using a simple mapping\n        vocab = {\n            "go": 1, "to": 2, "the": 3, "red": 4, "box": 5,\n            "pick": 6, "up": 7, "blue": 8, "ball": 9, "left": 10,\n            "right": 11, "front": 12, "behind": 13, "near": 14\n        }\n\n        tokens = []\n        for word in command.lower().split():\n            tokens.append(vocab.get(word, 0))  # 0 for unknown words\n\n        return tokens\n'})}),"\n",(0,o.jsx)(n.h2,{id:"exercise-implement-scene-aware-command-following",children:"Exercise: Implement Scene-Aware Command Following"}),"\n",(0,o.jsx)(n.p,{children:"Create a system that:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Processes visual input to understand the scene"}),"\n",(0,o.jsx)(n.li,{children:"Interprets natural language commands in visual context"}),"\n",(0,o.jsx)(n.li,{children:"Executes appropriate actions based on grounded understanding"}),"\n",(0,o.jsx)(n.li,{children:"Provides feedback about the action taken"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Implementing vision-language models for robotics requires careful consideration of real-time performance, computational efficiency, and the integration of visual and linguistic information. The models must be able to understand complex visual scenes, interpret natural language commands, and execute appropriate actions in the physical world. Grounded language understanding is crucial for enabling robots to follow complex instructions in their environment."}),"\n",(0,o.jsx)(n.hr,{})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);